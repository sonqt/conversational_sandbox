{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists at /Volumes/Users/tran_s2/.convokit/downloads/conversations-gone-awry-corpus\n"
     ]
    }
   ],
   "source": [
    "from convokit import Corpus, download\n",
    "corpus = Corpus(filename=download(\"conversations-gone-awry-corpus\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadPairs(corpus, split=None, last_only=False):\n",
    "    \"\"\"\n",
    "    Load context-reply pairs from the Corpus, optionally filtering to only conversations\n",
    "    from the specified split (train, val, or test).\n",
    "    Each conversation, which has N comments (not including the section header) will\n",
    "    get converted into N-1 comment-reply pairs, one pair for each reply\n",
    "    (the first comment does not reply to anything).\n",
    "    Each comment-reply pair is a tuple consisting of the conversational context\n",
    "    (that is, all comments prior to the reply), the reply itself, the label (that\n",
    "    is, whether the reply contained a derailment event), and the comment ID of the\n",
    "    reply (for later use in re-joining with the ConvoKit corpus).\n",
    "    The function returns a list of such pairs.\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    count_attack = 0\n",
    "    count_convo = 0\n",
    "    for convo in corpus.iter_conversations():\n",
    "        # consider only conversations in the specified split of the data\n",
    "        if split is None or convo.meta['split'] == split:\n",
    "            count_convo += 1\n",
    "            utterance_list = []\n",
    "            for utterance in convo.iter_utterances():\n",
    "                if utterance.meta['is_section_header']:\n",
    "                    continue\n",
    "                if utterance.meta['comment_has_personal_attack']:\n",
    "                    count_attack += 1\n",
    "                utterance_list.append({\"text\": utterance.text, \n",
    "                                        \"is_attack\": int(utterance.meta['comment_has_personal_attack']), \n",
    "                                        \"id\": utterance.id})\n",
    "                \n",
    "            iter_range = range(1, len(utterance_list)) if not last_only else [len(utterance_list)-1]\n",
    "            for idx in iter_range:\n",
    "                reply = utterance_list[idx][\"text\"]\n",
    "                label = utterance_list[idx][\"is_attack\"]\n",
    "                comment_id = utterance_list[idx][\"id\"]\n",
    "                # gather as context all utterances preceding the reply\n",
    "                context = [u[\"text\"] for u in utterance_list[:idx]]\n",
    "                pairs.append((context, reply, label, comment_id))\n",
    "\n",
    "    return pairs\n",
    "def conversations2utterances(conversations):\n",
    "    \"\"\"\n",
    "    Convert list of conversations into list of utterances for UtteranceModel.\n",
    "    INPUT:\n",
    "        conversations: list of list of str\n",
    "            List of conversations, each conversation is a list of utterances.\n",
    "    OUTPUT:\n",
    "        utterances: list of str\n",
    "            List of utterances in the dataset.\n",
    "        conversationLength: list of int\n",
    "            List of number of utterances in conversations.\n",
    "    \"\"\"\n",
    "    conversationLength = [len(convo) for convo in conversations]\n",
    "    utterances = []\n",
    "    for convo in conversations:\n",
    "        for utterance in convo:\n",
    "            utterances.append(utterance)\n",
    "    # assert len(utterances) == sum(conversationLength)\n",
    "    return utterances, conversationLength\n",
    "def load_data(corpus, context_batch_size = 32, split=None, last_only=False, shuffle=True):\n",
    "    \"\"\"\n",
    "    Load data from corpus into the format ready for UtteranceModel.\n",
    "    INPUT:\n",
    "        corpus: convokit.Corpus\n",
    "        split: str, optional\n",
    "            If specified, only consider conversations in the specified split of the data.\n",
    "        last_only: bool, optional\n",
    "            If True, only consider the last utterance in each conversation.\n",
    "    OUTPUT:\n",
    "        utterances: list of str\n",
    "            List of utterances in the dataset.\n",
    "        conversationLength: list of int\n",
    "            List of lengths of conversations in the dataset.\n",
    "        comment_ids: list of str\n",
    "            List of ids corresponding to the reply utterance.\n",
    "        labels: list of int\n",
    "            List of labels for each context if the next reply contains personal attack.\n",
    "    \"\"\"\n",
    "    pairs = loadPairs(corpus, split, last_only)\n",
    "    if shuffle:\n",
    "        random.shuffle(pairs)\n",
    "    batch_labels = []\n",
    "    batch_comment_ids = []\n",
    "    batch_utterances = []\n",
    "    batch_conversationLength = []\n",
    "    conversations = []\n",
    "    labels = []\n",
    "    comment_ids = []\n",
    "    for pair in pairs:\n",
    "        if len(labels) == context_batch_size:\n",
    "            utterances, conversationLength = conversations2utterances(conversations)\n",
    "            batch_utterances.append(utterances)\n",
    "            batch_conversationLength.append(conversationLength)\n",
    "            batch_labels.append(labels)\n",
    "            batch_comment_ids.append(comment_ids)\n",
    "            assert len(conversationLength) == len(comment_ids) == len(labels)\n",
    "            conversations = []\n",
    "            labels = []\n",
    "            comment_ids = []\n",
    "\n",
    "        context, _, label, comment_id = pair\n",
    "        conversations.append(context)\n",
    "        labels.append(label)\n",
    "        comment_ids.append(comment_id)\n",
    "    if len(conversations) > 0:\n",
    "        utterances, conversationLength = conversations2utterances(conversations)\n",
    "        batch_utterances.append(utterances)\n",
    "        batch_conversationLength.append(conversationLength)\n",
    "        batch_labels.append(labels)\n",
    "        batch_comment_ids.append(comment_ids)\n",
    "    return batch_utterances, batch_conversationLength, batch_comment_ids, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT utterance encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/Users/tran_s2/.local/lib/python3.11/site-packages/huggingface_hub/utils/_runtime.py:184: UserWarning: Pydantic is installed but cannot be imported. Please check your installation. `huggingface_hub` will default to not using Pydantic. Error message: '{e}'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaPreTrainedModel\n",
    "from transformers import RobertaTokenizer\n",
    "from transformers import RobertaModel\n",
    "from torch import nn\n",
    "class RoBERTaForUtterance(RobertaPreTrainedModel):\n",
    "    def __init__(self, config, device, batch_size=4):\n",
    "        super(RoBERTaForUtterance, self).__init__(config)\n",
    "        self.batch_size = batch_size\n",
    "        self.roberta = RobertaModel.from_pretrained(\"roberta-base\").to(device)\n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "        # self.attention1 = nn.Linear(768, 768)\n",
    "        # self.attention2 = nn.Linear(768, 1, bias=False)\n",
    "        # self.clf = nn.Linear(768, 1)\n",
    "    def tokenize(self, utterances, conversationLength):\n",
    "        # curr_utterance_idx = 0\n",
    "        # first_col = []\n",
    "        # second_col = []\n",
    "        # for convo_len in conversationLength:\n",
    "        #     first_col.append(\"Start\")\n",
    "        #     second_col.append(utterances[curr_utterance_idx])\n",
    "        #     for i in range(curr_utterance_idx + 1, curr_utterance_idx + convo_len):\n",
    "        #         first_col.append(utterances[i-1])\n",
    "        #         second_col.append(utterances[i])\n",
    "        #     curr_utterance_idx += convo_len\n",
    "\n",
    "        # def batch_tokenize(first_col, second_col, batch_size=8):\n",
    "        #     curr_idx = 0\n",
    "        #     while curr_idx < len(first_col):\n",
    "        #         tokens = self.tokenizer(first_col[curr_idx:curr_idx+batch_size], \n",
    "        #                         second_col[curr_idx:curr_idx+batch_size], \n",
    "        #                             padding=\"max_length\", truncation='longest_first', \n",
    "        #                             max_length=512, return_tensors=\"pt\")\n",
    "        #         yield tokens\n",
    "        #         curr_idx += batch_size\n",
    "        def batch_tokenize(utterances, batch_size=8):\n",
    "            curr_idx = 0\n",
    "            while curr_idx < len(utterances):\n",
    "                tokens = self.tokenizer(utterances[curr_idx:curr_idx+batch_size], \n",
    "                                padding=\"max_length\", truncation=True, \n",
    "                                max_length=256, return_tensors=\"pt\")\n",
    "                yield tokens\n",
    "                curr_idx += batch_size\n",
    "        return batch_tokenize(utterances, batch_size=self.batch_size)\n",
    "\n",
    "    def forward(self, utterances, conversationLength):\n",
    "        cls_vectors = []\n",
    "        for tokens in self.tokenize(utterances, conversationLength):\n",
    "            outputs = self.roberta(tokens.input_ids.to(device),\n",
    "                        attention_mask=tokens.attention_mask.to(device))\n",
    "            hidden = outputs.pooler_output.cpu().detach()\n",
    "            \n",
    "            # cls = hidden[:, 0, :].squeeze(1)\n",
    "            cls_vectors.append(hidden)\n",
    "        return torch.cat(cls_vectors, dim=0)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextEncoderRNN(nn.Module):\n",
    "    \"\"\"This module represents the context encoder component of CRAFT, responsible for creating an order-sensitive vector representation of conversation context\"\"\"\n",
    "    def __init__(self, hidden_size, n_layers=1, dropout=0):\n",
    "        super(ContextEncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # only unidirectional GRU for context encoding\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, batch_first = True,\n",
    "                          dropout=(0 if n_layers == 1 else dropout), bidirectional=False)\n",
    "\n",
    "    def forward(self, input_seq, hidden=None):\n",
    "        # Pack padded batch of sequences for RNN module\n",
    "        # packed = torch.nn.utils.rnn.pack_padded_sequence(input_seq, input_lengths)\n",
    "        # Forward pass through GRU\n",
    "        outputs, hidden = self.gru(input_seq, hidden)\n",
    "        # Unpack padding\n",
    "        # outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        # return output and final hidden state\n",
    "        return outputs, hidden\n",
    "\n",
    "class SingleTargetClf(nn.Module):\n",
    "    \"\"\"This module represents the CRAFT classifier head, which takes the context encoding and uses it to make a forecast\"\"\"\n",
    "    def __init__(self, hidden_size, dropout=0.1):\n",
    "        super(SingleTargetClf, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # initialize classifier\n",
    "        self.layer1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.layer1_act = nn.LeakyReLU()\n",
    "        self.layer2 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.layer2_act = nn.LeakyReLU()\n",
    "        self.clf = nn.Linear(hidden_size // 2, 1)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    def forward(self, encoder_hidden):\n",
    "        hidden = encoder_hidden[-1,:,:]\n",
    "        # forward pass through hidden layers\n",
    "        hidden = hidden.squeeze()\n",
    "        layer1_out = self.layer1_act(self.layer1(self.dropout(hidden)))\n",
    "        layer2_out = self.layer2_act(self.layer2(self.dropout(layer1_out)))\n",
    "        # compute and return logits\n",
    "        logits = self.clf(self.dropout(layer2_out)).squeeze()\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierAtt(nn.Module):\n",
    "    def __init__(self, utt_emb_size) -> None:\n",
    "        super().__init__()\n",
    "        self.attention1 = nn.Linear(utt_emb_size, utt_emb_size)\n",
    "        self.attention2 = nn.Linear(utt_emb_size, 1, bias=False)\n",
    "        self.clf = nn.Linear(utt_emb_size, 1)\n",
    "    def forward(self, utt_emb, mask):\n",
    "        # utt_emb: [batch_size, seq_length, utt_emb_size]\n",
    "        conversation_embedding = self.attention_net(utt_emb, self.attention1, self.attention2, mask)\n",
    "        final_output = self.clf(conversation_embedding)\n",
    "        return final_output\n",
    "\n",
    "    def attention_net(self, utt_emb, attention_net1, attention_net2 , mask):\n",
    "        print(utt_emb.get_device())\n",
    "        print(attention_net1.get_device())\n",
    "        hidden_re = torch.tanh(attention_net1(utt_emb)) # [batch_size, seq_length, utt_emb_size]\n",
    "        attn_weights = attention_net2(hidden_re).squeeze(2) # [batch_size, seq_length]\n",
    "        attn_weights = attn_weights.masked_fill(mask==0, -1e15)\n",
    "        soft_attn_weights = F.softmax(attn_weights, 1) # [batch_size, seq_length]\n",
    "        # [batch_size, utt_emb_size, seq_length] * [batch_size, seq_length, 1] = [batch_size, utt_emb_size, 1]\n",
    "        final_embedding = torch.bmm(utt_emb.transpose(1, 2), soft_attn_weights.unsqueeze(2)).squeeze(2)\n",
    "        return final_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_utterances, train_conversationLength, train_comment_ids, train_labels = load_data(corpus, split='train', last_only=True, context_batch_size=64)\n",
    "valid_utterances, valid_conversationLength, valid_comment_ids, valid_labels = load_data(corpus, split='val', last_only=True, context_batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.453125"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(valid_labels[0])/len(valid_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "config = AutoConfig.from_pretrained('roberta-base')\n",
    "my_roberta = RoBERTaForUtterance(config, 'cuda', batch_size=16)\n",
    "context_encoder = ContextEncoderRNN(768, 1, 0.1)\n",
    "attack_clf = SingleTargetClf(768, 0.1)\n",
    "context_encoder = context_encoder.to(device)\n",
    "attack_clf = attack_clf.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def prepare_context_batch(utt_hidden, batch_conversationLength, max_context_len=20):\n",
    "    assert utt_hidden.shape[0] == sum(batch_conversationLength)\n",
    "    # utt_encoder_summed = utt_hidden[-2,:,:] + utt_hidden[-1,:,:]\n",
    "    hidden_size = utt_hidden.shape[1]\n",
    "    context_features = np.zeros((len(batch_conversationLength), max_context_len, hidden_size), dtype=np.float32)\n",
    "\n",
    "    current_utt_idx = 0\n",
    "    for i, convo_len in enumerate(batch_conversationLength):\n",
    "        if convo_len > max_context_len:\n",
    "            current_utt_idx += convo_len - max_context_len\n",
    "            convo_len = max_context_len\n",
    "        context_features[i, -convo_len:, :] = np.array(utt_hidden)[current_utt_idx:current_utt_idx+convo_len, :]\n",
    "        current_utt_idx += convo_len\n",
    "    return context_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "learning_rate = 3e-5\n",
    "# pos_weight = torch.tensor([0.8]).to(device)\n",
    "# criterion = nn.BCEWithLogitsLoss(reduction = 'sum')\n",
    "# loss = loss_fct(logits, labels)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "# encoder_optimizer = optim.Adam(my_roberta.parameters(), lr=learning_rate)\n",
    "encoder_optimizer = optim.AdamW(my_roberta.parameters(),\n",
    "                  lr = learning_rate, # args.learning_rate - default is 5e-5,\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n",
    "context_encoder_optimizer = optim.AdamW(context_encoder.parameters(), lr=learning_rate)\n",
    "attack_clf_optimizer = optim.AdamW(attack_clf.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "347\n"
     ]
    }
   ],
   "source": [
    "batch_utterances = train_utterances[0]\n",
    "batch_conversationLength = train_conversationLength[0]\n",
    "print(len(batch_utterances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f1_score(labels, preds):\n",
    "    # preds = torch.sigmoid(logits) > 0.5\n",
    "    # Calculating precision, recall, and F1 score using PyTorch\n",
    "    TP = ((preds == 1) & (labels == 1)).sum().item()\n",
    "    FP = ((preds == 1) & (labels == 0)).sum().item()\n",
    "    FN = ((preds == 0) & (labels == 1)).sum().item()\n",
    "\n",
    "    precision = TP / (TP + FP) if TP + FP > 0 else 0\n",
    "    recall = TP / (TP + FN) if TP + FN > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return f1\n",
    "def evaluate(encoder, context_encoder, attack_clf, val_utterances, val_conversationLength, val_labels):\n",
    "    encoder.eval()\n",
    "    context_encoder.eval()\n",
    "    attack_clf.eval()\n",
    "    val_loss = 0\n",
    "    val_f1 = 0\n",
    "    num_sample = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    for i in range(len(val_utterances)):\n",
    "        batch_utterances = val_utterances[i]\n",
    "        batch_conversationLength = val_conversationLength[i]\n",
    "        batch_labels = val_labels[i]\n",
    "        batch_size = len(batch_labels)\n",
    "        num_sample += batch_size\n",
    "        if batch_size == 0:\n",
    "            continue\n",
    "        with torch.no_grad():\n",
    "            utt_hidden = encoder.forward(batch_utterances, batch_conversationLength)\n",
    "            context_features = prepare_context_batch(utt_hidden, batch_conversationLength)\n",
    "            context_features = torch.from_numpy(context_features).to(device)\n",
    "            context_outputs, context_hidden = context_encoder(context_features)\n",
    "            logits = attack_clf(context_hidden)\n",
    "            labels = torch.tensor(batch_labels, dtype=torch.float32).to(device)\n",
    "            # pos_weight = torch.tensor([1]).type_as(logits)\n",
    "            # loss_fct = BCEWithLogitsLoss(pos_weight=pos_weight, reduction = 'sum')\n",
    "            # loss = loss_fct(logits, labels)\n",
    "            loss = F.binary_cross_entropy_with_logits(logits, labels)\n",
    "            val_loss += loss.item()\n",
    "            preds = torch.sigmoid(logits) > 0.5\n",
    "            # pos += (preds.sum().item() / batch_size)\n",
    "            all_labels.append(labels.cpu().detach())\n",
    "            all_preds.append(preds.cpu().detach())\n",
    "            # val_accuracy += (preds == labels).sum().item() / batch_size\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "    all_preds = torch.cat(all_preds, dim=0)\n",
    "    # print(num_sample, all_labels.shape, all_preds.shape)\n",
    "    assert all_labels.shape == all_preds.shape\n",
    "    # assert all_labels.shape[0] == num_sample\n",
    "    val_accuracy = (all_labels == all_preds).sum().item()/num_sample\n",
    "    pos = all_preds.sum().item()/num_sample\n",
    "\n",
    "    val_f1 = calculate_f1_score(all_labels, all_preds)\n",
    "    return val_loss, val_f1, val_accuracy, pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 4\n",
      "Validation loss: 9.66 accuracy: 57.14 f1: 50.55 pos: 0.37\n",
      "400 9\n",
      "Validation loss: 9.59 accuracy: 54.76 f1: 39.10 pos: 0.24\n",
      "600 14\n",
      "Validation loss: 9.56 accuracy: 57.02 f1: 52.06 pos: 0.40\n",
      "800 19\n",
      "Validation loss: 9.55 accuracy: 57.02 f1: 52.06 pos: 0.40\n",
      "1000 24\n",
      "Validation loss: 9.56 accuracy: 57.02 f1: 52.06 pos: 0.40\n",
      "1200 29\n",
      "Validation loss: 9.54 accuracy: 57.02 f1: 52.06 pos: 0.40\n",
      "1400 34\n",
      "Validation loss: 9.54 accuracy: 56.79 f1: 51.66 pos: 0.39\n",
      "1600 39\n",
      "Validation loss: 9.55 accuracy: 57.14 f1: 51.87 pos: 0.39\n",
      "1800 44\n",
      "Validation loss: 9.55 accuracy: 57.14 f1: 51.87 pos: 0.39\n",
      "2000 49\n",
      "Validation loss: 9.55 accuracy: 57.02 f1: 51.41 pos: 0.38\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.disable(logging.WARNING)\n",
    "num_steps = 0\n",
    "for epoch in range(50):\n",
    "    for batch_idx in range(len(train_labels)):\n",
    "        num_steps += 1\n",
    "        if num_steps % 200 == 0:\n",
    "            print(num_steps, epoch)\n",
    "            val_loss, val_f1, val_accuracy, pos= evaluate(my_roberta, context_encoder, attack_clf, valid_utterances, valid_conversationLength, valid_labels)\n",
    "            print(\"Validation loss: {:.2f} accuracy: {:.2f} f1: {:.2f} pos: {:.2f}\".format(val_loss, val_accuracy * 100, val_f1 * 100, pos))\n",
    "\n",
    "        my_roberta.train()\n",
    "        context_encoder.train()\n",
    "        attack_clf.train()\n",
    "        \n",
    "        batch_utterances = train_utterances[batch_idx]\n",
    "        batch_conversationLength = train_conversationLength[batch_idx]\n",
    "        batch_comment_ids = train_comment_ids[batch_idx]\n",
    "        batch_labels = train_labels[batch_idx]\n",
    "        encoder_optimizer.zero_grad()\n",
    "        context_encoder_optimizer.zero_grad()\n",
    "        attack_clf_optimizer.zero_grad()\n",
    "        \n",
    "        hidden = my_roberta.forward(batch_utterances, batch_conversationLength)\n",
    "        context_features = torch.from_numpy(prepare_context_batch(hidden, batch_conversationLength)).to(device)\n",
    "        final_outputs, final_hidden = context_encoder.forward(context_features)\n",
    "        logits = attack_clf(final_hidden)\n",
    "        labels = torch.tensor(batch_labels, dtype=torch.float32).to(device)\n",
    "\n",
    "        # loss = F.binary_cross_entropy_with_logits(logits, labels)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        clip = 50.0\n",
    "        # Clip gradients: gradients are modified in place\n",
    "        _ = torch.nn.utils.clip_grad_norm_(my_roberta.parameters(), clip)\n",
    "        _ = torch.nn.utils.clip_grad_norm_(context_encoder.parameters(), clip)\n",
    "        _ = torch.nn.utils.clip_grad_norm_(attack_clf.parameters(), clip)\n",
    "\n",
    "        # Adjust model weights\n",
    "        encoder_optimizer.step()\n",
    "        context_encoder_optimizer.step()\n",
    "        attack_clf_optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
