{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5b749c0-7600-43ba-88b9-e0809a4b29aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from convokit import download, Corpus\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e4a17bd-ba9e-4755-bada-c2b36f1220f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists at /home/sqt2/.convokit/downloads/conversations-gone-awry-corpus\n",
      "Dataset already exists at /home/sqt2/.convokit/downloads/conversations-gone-awry-cmv-corpus\n"
     ]
    }
   ],
   "source": [
    "full_model_path = \"/reef/sqt2/BERTCRAFT\"\n",
    "single_model_path = \"/reef/sqt2/SINGLE_UTT\"\n",
    "\n",
    "wikicorpus = Corpus(filename=download(\"conversations-gone-awry-corpus\"))\n",
    "cmvcorpus = Corpus(filename=download(\"conversations-gone-awry-cmv-corpus\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7323ac49-4732-44e3-9e57-8cb7becb45b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_test(test_samples, pred_path, corpus, corpus_name):\n",
    "    all_logits = []\n",
    "    label_metadata = \"conversation_has_personal_attack\" if corpus_name == \"wikiconv\" else \"has_removed_comment\"\n",
    "    pred_file = open(pred_path, 'r')\n",
    "    pred_lines = pred_file.readlines()[1:]\n",
    "    pred_dict = {}\n",
    "    for line in pred_lines:\n",
    "        id2pred = line.split(\",\")\n",
    "        \n",
    "        assert len(id2pred) == 3\n",
    "        utt_id = id2pred[0]\n",
    "        utt_pred = int(float(id2pred[2]) > 0.4)\n",
    "        utt_score = id2pred[2]\n",
    "        all_logits.append(float(utt_score))\n",
    "        pred_dict[utt_id] = [int(utt_pred), float(utt_score)]\n",
    "    # plt.hist(all_logits, bins=20)\n",
    "    # plt.show()\n",
    "    for convo in corpus.iter_conversations():\n",
    "        prev = 0\n",
    "        # only consider test set conversations (we did not make predictions for the other ones)\n",
    "        if convo.id in test_samples:\n",
    "            for utt in convo.iter_utterances():\n",
    "                if utt.id in pred_dict:\n",
    "                    utt.meta['forecast_score'] = (pred_dict[utt.id][0] * prev) or (pred_dict[utt.id][1] > 0.8)\n",
    "                    prev = pred_dict[utt.id][0]\n",
    "    \n",
    "    conversational_forecasts_df = {\n",
    "            \"convo_id\": [],\n",
    "            \"label\": [],\n",
    "            \"prediction\": []\n",
    "        }\n",
    "    for convo in corpus.iter_conversations():\n",
    "        if convo.id in test_samples:\n",
    "            conversational_forecasts_df['convo_id'].append(convo.id)\n",
    "            conversational_forecasts_df['label'].append(int(convo.meta[label_metadata]))\n",
    "            forecast_scores = [utt.meta['forecast_score'] for utt in convo.iter_utterances() if 'forecast_score' in utt.meta]\n",
    "            conversational_forecasts_df['prediction'].append(max(forecast_scores))\n",
    "    conversational_forecasts_df = pd.DataFrame(conversational_forecasts_df).set_index(\"convo_id\")\n",
    "    test_labels = conversational_forecasts_df.label\n",
    "    test_preds = conversational_forecasts_df.prediction\n",
    "    test_acc = (test_labels == test_preds).mean()\n",
    "    \n",
    "    tp = ((test_labels==1)&(test_preds==1)).sum()\n",
    "    fp = ((test_labels==0)&(test_preds==1)).sum()\n",
    "    tn = ((test_labels==0)&(test_preds==0)).sum()\n",
    "    fn = ((test_labels==1)&(test_preds==0)).sum()\n",
    "\n",
    "    test_precision = tp / (tp + fp)\n",
    "    test_recall = tp / (tp + fn)\n",
    "    test_fpr = fp / (fp + tn)\n",
    "    test_f1 = 2 / (((tp + fp) / tp) + ((tp + fn) / tp))\n",
    "    return {\"accuracy\":test_acc, \"precision\":test_precision, \"recall\":test_recall, \"f1\":test_f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "26691f94-2dda-4e78-afad-20490ed8ef5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_utt_preds(saved_path):\n",
    "    single_utt_predictions = {}\n",
    "    all_seeds = os.listdir(saved_path)\n",
    "    for seed in all_seeds:\n",
    "        pred_path = os.path.join(saved_path, seed, \"predictions.csv\")\n",
    "        pred_file = open(pred_path, 'r')\n",
    "        pred_lines = pred_file.readlines()[1:]\n",
    "        for line in pred_lines:\n",
    "            id2pred = line.split(\",\")\n",
    "            \n",
    "            assert len(id2pred) == 3\n",
    "            utt_id = id2pred[0]\n",
    "            utt_pred = id2pred[1]\n",
    "            if utt_id not in single_utt_predictions:\n",
    "                single_utt_predictions[utt_id] = int(utt_pred)\n",
    "            else:\n",
    "                single_utt_predictions[utt_id] += int(utt_pred)\n",
    "    return single_utt_predictions\n",
    "    \n",
    "def full_evaluate(full_model_name, full_model_path, single_model_name, single_model_path, corpus, corpus_name):\n",
    "    single_model_path = os.path.join(single_model_path, corpus_name, single_model_name)\n",
    "    full_model_path = os.path.join(full_model_path, corpus_name, full_model_name)\n",
    "    \n",
    "    single_utt_predictions = get_single_utt_preds(single_model_path)\n",
    "    test_samples, dynamic_samples, single_samples = extract_dynamic_samples(single_utt_predictions, corpus, corpus_name)\n",
    "\n",
    "    with open('{}.txt'.format(corpus_name), 'w') as f:\n",
    "        for id in dynamic_samples:\n",
    "            f.write(\"%s\\n\" % id)\n",
    "\n",
    "    result_dict = {\"full_test\": {\"accuracy\":[], \"precision\":[], \"recall\":[], \"f1\":[]},\n",
    "                  \"dynamic_only\": {\"accuracy\":[], \"precision\":[], \"recall\":[], \"f1\":[]},\n",
    "                  \"single_enough\": {\"accuracy\":[], \"precision\":[], \"recall\":[], \"f1\":[]}}\n",
    "\n",
    "    for seed in range(1,11):        \n",
    "        pred_path = os.path.join(full_model_path, \"seed-{}\".format(seed), \"predictions.csv\")\n",
    "        full_test = new_test(test_samples, pred_path, corpus, corpus_name)\n",
    "        for metric in full_test:\n",
    "            result_dict['full_test'][metric].append(full_test[metric])\n",
    "        dynamic_only = new_test(dynamic_samples, pred_path, corpus, corpus_name)\n",
    "        for metric in dynamic_only:\n",
    "            result_dict['dynamic_only'][metric].append(dynamic_only[metric])\n",
    "        single_enough = new_test(single_samples, pred_path, corpus, corpus_name)\n",
    "        for metric in single_enough:\n",
    "            result_dict['single_enough'][metric].append(single_enough[metric])\n",
    "    for metric in result_dict['full_test']:\n",
    "        result_dict['full_test'][metric] = np.mean(result_dict['full_test'][metric])\n",
    "        result_dict['dynamic_only'][metric] = np.mean(result_dict['dynamic_only'][metric])\n",
    "        result_dict['single_enough'][metric] = np.mean(result_dict['single_enough'][metric])\n",
    "    return result_dict\n",
    "def extract_dynamic_samples(all_predictions, corpus, corpus_name):\n",
    "    label_metadata = \"conversation_has_personal_attack\" if corpus_name == \"wikiconv\" else \"has_removed_comment\"\n",
    "    num_convo = 0\n",
    "    hard_pos, hard_neg = 0, 0\n",
    "    all_pos, all_neg = 0, 0\n",
    "    dynamic_samples = []\n",
    "    test_samples = []\n",
    "    for convo in corpus.iter_conversations():\n",
    "        if convo.meta['split'] == 'test':\n",
    "            test_samples.append(convo.id)\n",
    "            max_agreement = 0\n",
    "            for utterance in convo.iter_utterances():\n",
    "                id = utterance.id\n",
    "                if id in all_predictions:\n",
    "                    if all_predictions[id] > max_agreement:\n",
    "                        max_agreement = all_predictions[id]\n",
    "            if convo.meta[label_metadata] == False:\n",
    "                all_neg += 1\n",
    "                if max_agreement >= 3:\n",
    "                    hard_neg += 1\n",
    "                    dynamic_samples.append(convo.id)\n",
    "            else:\n",
    "                all_pos += 1\n",
    "                if max_agreement <= 8:\n",
    "                    hard_pos += 1\n",
    "                    dynamic_samples.append(convo.id)\n",
    "    print(\"We have {} positive samples and {} negative samples in the test set\".format(all_pos, all_neg))\n",
    "    # print(\"We have {} positive samples and {} negative dynamic samples\".format(hard_pos, hard_neg))\n",
    "\n",
    "    single_samples = [id for id in test_samples if id not in dynamic_samples]\n",
    "    print(len(dynamic_samples))\n",
    "    print(len(single_samples))\n",
    "    return test_samples, dynamic_samples, single_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "08d4d98a-b5a6-4677-ac31-da7124ae2bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 420 positive samples and 420 negative samples in the test set\n",
      "420\n",
      "420\n",
      "{'full_test': {'accuracy': 0.6488095238095238, 'precision': 0.6419128579597747, 'recall': 0.6757142857142857, 'f1': 0.6573810572690272}, 'dynamic_only': {'accuracy': 0.4133333333333333, 'precision': 0.4284775098524071, 'recall': 0.46261682242990654, 'f1': 0.44372206824574717}, 'single_enough': {'accuracy': 0.8842857142857141, 'precision': 0.8719198117095093, 'recall': 0.8970873786407767, 'f1': 0.8836917831036912}}\n"
     ]
    }
   ],
   "source": [
    "print(full_evaluate(\"roberta-base\", full_model_path, \"roberta-base\", single_model_path, wikicorpus, 'wikiconv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "66977766-3bd8-4398-9c4f-72da5111e7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 684 positive samples and 684 negative samples in the test set\n",
      "643\n",
      "725\n",
      "{'full_test': {'accuracy': 0.6628654970760233, 'precision': 0.6472230110242035, 'recall': 0.7290935672514619, 'f1': 0.6826342611722096}, 'dynamic_only': {'accuracy': 0.40979782270606535, 'precision': 0.48020863892416116, 'recall': 0.5335149863760218, 'f1': 0.5026072874705325}, 'single_enough': {'accuracy': 0.8873103448275861, 'precision': 0.8273038729813302, 'recall': 0.9555205047318612, 'f1': 0.8838040981592844}}\n"
     ]
    }
   ],
   "source": [
    "print(full_evaluate(\"roberta-base\", full_model_path, \"roberta-base\", single_model_path, cmvcorpus, 'cmv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dcc69e-0501-4072-b7ae-29fcec5d5019",
   "metadata": {},
   "source": [
    "# 2 Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f54d4610-89f1-4401-a3ed-3818d5ff0eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists at /home/sqt2/.convokit/downloads/conversations-gone-awry-corpus\n"
     ]
    }
   ],
   "source": [
    "from convokit import download, Corpus\n",
    "wikicorpus = Corpus(filename=download(\"conversations-gone-awry-corpus\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "13befbe4-131a-40e9-bfb3-b54dfdddeb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "def tune_thresholds(val_pred_path, corpus, corpus_name):\n",
    "    label_metadata = \"conversation_has_personal_attack\" if corpus_name == \"wikiconv\" else \"has_removed_comment\"\n",
    "\n",
    "    pred_file = open(val_pred_path, 'r')\n",
    "    pred_lines = pred_file.readlines()[1:]\n",
    "    pred_dict = {}\n",
    "\n",
    "    best_acc = 0\n",
    "    bestthres_set = [0, 0]\n",
    "    for first_thres in np.arange(0.3,0.5,0.01):\n",
    "        for second_thres in np.arange(0.5,0.85, 0.01):\n",
    "            for line in pred_lines:\n",
    "                id2pred = line.split(\",\")\n",
    "                \n",
    "                assert len(id2pred) == 3\n",
    "                utt_id = id2pred[0]\n",
    "                utt_pred = int(float(id2pred[2]) > first_thres)\n",
    "                utt_score = id2pred[2]\n",
    "                # all_logits.append(float(utt_score))\n",
    "                pred_dict[utt_id] = [int(utt_pred), float(utt_score)]\n",
    "            # plt.hist(all_logits, bins=20)\n",
    "            # plt.show()\n",
    "            for convo in corpus.iter_conversations():\n",
    "                prev = 0\n",
    "                # only consider test set conversations (we did not make predictions for the other ones)\n",
    "                if convo.meta['split'] == 'val':\n",
    "                    for utt in convo.iter_utterances():\n",
    "                        if utt.id in pred_dict:\n",
    "                            utt.meta['forecast_score'] = (pred_dict[utt.id][0] * prev) or (pred_dict[utt.id][1] > second_thres)\n",
    "                            prev = prev or pred_dict[utt.id][0]\n",
    "            \n",
    "            conversational_forecasts_df = {\n",
    "                    \"convo_id\": [],\n",
    "                    \"label\": [],\n",
    "                    \"prediction\": []\n",
    "                }\n",
    "            for convo in corpus.iter_conversations():\n",
    "                if convo.meta['split'] == 'val':\n",
    "                    conversational_forecasts_df['convo_id'].append(convo.id)\n",
    "                    conversational_forecasts_df['label'].append(int(convo.meta[label_metadata]))\n",
    "                    forecast_scores = [utt.meta['forecast_score'] for utt in convo.iter_utterances() if 'forecast_score' in utt.meta]\n",
    "                    conversational_forecasts_df['prediction'].append(max(forecast_scores))\n",
    "            conversational_forecasts_df = pd.DataFrame(conversational_forecasts_df).set_index(\"convo_id\")\n",
    "            test_labels = conversational_forecasts_df.label\n",
    "            test_preds = conversational_forecasts_df.prediction\n",
    "            test_acc = (test_labels == test_preds).mean()\n",
    "            \n",
    "            tp = ((test_labels==1)&(test_preds==1)).sum()\n",
    "            fp = ((test_labels==0)&(test_preds==1)).sum()\n",
    "            tn = ((test_labels==0)&(test_preds==0)).sum()\n",
    "            fn = ((test_labels==1)&(test_preds==0)).sum()\n",
    "        \n",
    "            test_precision = tp / (tp + fp)\n",
    "            test_recall = tp / (tp + fn)\n",
    "            test_fpr = fp / (fp + tn)\n",
    "            test_f1 = 2 / (((tp + fp) / tp) + ((tp + fn) / tp))\n",
    "            \n",
    "            if test_acc > best_acc:\n",
    "                best_acc = test_acc\n",
    "                bestthres_set = [first_thres, second_thres]\n",
    "    print(best_acc, bestthres_set)\n",
    "    return {\"accuracy\":test_acc, \"precision\":test_precision, \"recall\":test_recall, \"f1\":test_f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3991cf0a-da31-4a2c-b049-995e975ec2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6202380952380953 [0.49000000000000016, 0.7500000000000002]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6130952380952381,\n",
       " 'precision': 0.594059405940594,\n",
       " 'recall': 0.7142857142857143,\n",
       " 'f1': 0.6486486486486487}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tune_thresholds(\"/reef/sqt2/SINGLE_UTT_NEW/wikiconv/bert-base-cased/seed-1/pred_val.csv\", wikicorpus, 'wikiconv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168fd063-a57a-4730-9a30-e053565d921c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_thresholds(\"/reef/sqt2/SINGLE_UTT_NEW/wikiconv/bert-base-cased/seed-1/pred_test.csv\", wikicorpus, 'wikiconv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6c00ba-d744-405b-9af7-f9e76f3d6260",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tune_thresholds(\"/reef/sqt2/BERTCRAFT/wikiconv/roberta-base/seed-1/predictions.csv\", wikicorpus, 'wikiconv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jacq-zissou-env-3.11",
   "language": "python",
   "name": "jacq-zissou-env-3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
