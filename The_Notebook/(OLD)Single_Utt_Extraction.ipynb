{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63437823-3d21-4ce8-9e83-af50e9088d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "923fed86-84c2-4fb8-a516-7e551df0b3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_path = \"/reef/sqt2/SINGLE_UTT/cmv/roberta-base\"\n",
    "all_seeds = os.listdir(saved_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "544294c0-f1e8-4c11-a144-9d538440fd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions = {}\n",
    "for seed in all_seeds:\n",
    "    try:\n",
    "        pred_path = os.path.join(saved_path, seed, \"predictions.csv\")\n",
    "        pred_file = open(pred_path, 'r')\n",
    "        pred_lines = pred_file.readlines()[1:]\n",
    "        # print(pred_lines[0])\n",
    "        for line in pred_lines:\n",
    "            id2pred = line.split(\",\")\n",
    "            \n",
    "            assert len(id2pred) == 3\n",
    "            utt_id = id2pred[0]\n",
    "            utt_pred = id2pred[1]\n",
    "            if utt_id not in all_predictions:\n",
    "                all_predictions[utt_id] = int(utt_pred)\n",
    "            else:\n",
    "                all_predictions[utt_id] += int(utt_pred)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7635257-0405-43d7-a2f9-cd84b774d56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists at /home/sqt2/.convokit/downloads/conversations-gone-awry-cmv-corpus\n"
     ]
    }
   ],
   "source": [
    "from convokit import download, Corpus\n",
    "corpus = Corpus(filename=download(\"conversations-gone-awry-cmv-corpus\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ec5da90-a932-46fa-828f-1cb6903795ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{10: 363, 4: 49, 0: 419, 8: 73, 9: 91, 2: 65, 1: 100, 3: 58, 7: 56, 5: 41, 6: 53}\n",
      "243 284\n",
      "260\n"
     ]
    }
   ],
   "source": [
    "agreement_spectrum = {}\n",
    "true_agreement = 0\n",
    "num_convo = 0\n",
    "hard_att = 0\n",
    "hard_non = 0\n",
    "hard_samples = []\n",
    "for convo in corpus.iter_conversations():\n",
    "    if convo.meta['split'] == 'test':\n",
    "        num_convo += 1\n",
    "        max_agreement = 0\n",
    "        # print(convo)\n",
    "        for utterance in convo.iter_utterances():\n",
    "            id = utterance.id\n",
    "            if id in all_predictions:\n",
    "                if all_predictions[id] > max_agreement:\n",
    "                    max_agreement = all_predictions[id]\n",
    "        # Hard attack\n",
    "        if convo.meta['has_removed_comment'] == False:\n",
    "            if max_agreement >= 4:\n",
    "                hard_non += 1\n",
    "                hard_samples.append(convo.id)\n",
    "        else:\n",
    "            if max_agreement <= 6:\n",
    "                hard_att += 1\n",
    "                hard_samples.append(convo.id)\n",
    "        \n",
    "        if max_agreement ==10 and convo.meta['has_removed_comment'] == True:\n",
    "            true_agreement += 1\n",
    "        if max_agreement not in agreement_spectrum:\n",
    "            agreement_spectrum[max_agreement] = 1\n",
    "        else:\n",
    "            agreement_spectrum[max_agreement] += 1\n",
    "print(agreement_spectrum)\n",
    "print(hard_non, hard_att)\n",
    "print(true_agreement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e8ed7f2-bf44-4324-9621-73afd7aebf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "for derail_id in all_predictions:\n",
    "    if all_predictions[derail_id] == 10:\n",
    "        for convo in corpus.iter_conversations():\n",
    "            if convo.meta['split'] == 'test':\n",
    "                if convo.meta['has_removed_comment'] == False:\n",
    "                    for utterance in convo.iter_utterances():\n",
    "                        id = utterance.id\n",
    "                        # if id == derail_id:\n",
    "                            # print(utterance.text)\n",
    "                            #print next utterance here\n",
    "                            # print()\n",
    "                            # print()\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6dd833c-91ac-403b-bdd0-23b28ab588a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8466\n"
     ]
    }
   ],
   "source": [
    "pred_path = \"/reef/sqt2/BERTCRAFT/cmv/roberta-base/seed-1/predictions.csv\"\n",
    "pred_file = open(pred_path, 'r')\n",
    "pred_lines = pred_file.readlines()[1:]\n",
    "pred_dict = {}\n",
    "for line in pred_lines:\n",
    "    id2pred = line.split(\",\")\n",
    "    \n",
    "    assert len(id2pred) == 3\n",
    "    utt_id = id2pred[0]\n",
    "    utt_pred = id2pred[1]\n",
    "    pred_dict[utt_id] = int(utt_pred)\n",
    "print(len(pred_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7b045c9-93c9-4a56-a7ca-5cb5a12340e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_name = \"cmv\"\n",
    "label_metadata = \"conversation_has_personal_attack\" if corpus_name == \"wikiconv\" else \"has_removed_comment\"\n",
    "utt_label_metadata = \"comment_has_personal_attack\" if corpus_name == \"wikiconv\" else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "301c293d-d686-4519-a87b-77a2af652d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# print(pred_dict)\n",
    "def test_hard_samples(hard_samples, seed):\n",
    "    print(seed)\n",
    "    pred_path = \"/reef/sqt2/BERTCRAFT/cmv/roberta-base/seed-{}/predictions.csv\".format(seed)\n",
    "    pred_file = open(pred_path, 'r')\n",
    "    pred_lines = pred_file.readlines()[1:]\n",
    "    pred_dict = {}\n",
    "    for line in pred_lines:\n",
    "        id2pred = line.split(\",\")\n",
    "        \n",
    "        assert len(id2pred) == 3\n",
    "        utt_id = id2pred[0]\n",
    "        utt_pred = id2pred[1]\n",
    "        pred_dict[utt_id] = int(utt_pred)\n",
    "        \n",
    "    for convo in corpus.iter_conversations():\n",
    "        # only consider test set conversations (we did not make predictions for the other ones)\n",
    "        if convo.id in hard_samples:\n",
    "            for utt in convo.iter_utterances():\n",
    "                if utt.id in pred_dict:\n",
    "                    utt.meta['forecast_score'] = pred_dict[utt.id]\n",
    "    \n",
    "    conversational_forecasts_df = {\n",
    "            \"convo_id\": [],\n",
    "            \"label\": [],\n",
    "            \"prediction\": []\n",
    "        }\n",
    "    for convo in corpus.iter_conversations():\n",
    "        if convo.id in hard_samples:\n",
    "            conversational_forecasts_df['convo_id'].append(convo.id)\n",
    "            conversational_forecasts_df['label'].append(int(convo.meta[label_metadata]))\n",
    "            forecast_scores = [utt.meta['forecast_score'] for utt in convo.iter_utterances() if 'forecast_score' in utt.meta]\n",
    "            conversational_forecasts_df['prediction'].append(max(forecast_scores))\n",
    "    conversational_forecasts_df = pd.DataFrame(conversational_forecasts_df).set_index(\"convo_id\")\n",
    "    test_labels = conversational_forecasts_df.label\n",
    "    test_preds = conversational_forecasts_df.prediction\n",
    "    test_acc = (test_labels == test_preds).mean()\n",
    "    \n",
    "    \n",
    "    tp = ((test_labels==1)&(test_preds==1)).sum()\n",
    "    fp = ((test_labels==0)&(test_preds==1)).sum()\n",
    "    tn = ((test_labels==0)&(test_preds==0)).sum()\n",
    "    fn = ((test_labels==1)&(test_preds==0)).sum()\n",
    "\n",
    "    test_precision = tp / (tp + fp)\n",
    "    test_recall = tp / (tp + fn)\n",
    "    test_fpr = fp / (fp + tn)\n",
    "    test_f1 = 2 / (((tp + fp) / tp) + ((tp + fn) / tp))\n",
    "    return test_acc, test_precision, test_recall, test_f1\n",
    "    # print(\"\\tAccuracy:\", test_acc)\n",
    "    # print(\"\\tPrecision:\", test_precision)\n",
    "    # print(\"\\tRecall:\", test_recall)\n",
    "    # print(\"\\tF1:\", test_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "821b8720-d9d1-4f44-ad2f-198c58927dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\tAccuracy: 0.36622390891840606\n",
      "\tPrecision: 0.4301675977653631\n",
      "\tRecall: 0.5422535211267606\n",
      "\tF1: 0.47975077881619943\n",
      "2\n",
      "\tAccuracy: 0.3396584440227704\n",
      "\tPrecision: 0.39473684210526316\n",
      "\tRecall: 0.4225352112676056\n",
      "\tF1: 0.4081632653061224\n",
      "3\n",
      "\tAccuracy: 0.3301707779886148\n",
      "\tPrecision: 0.3925233644859813\n",
      "\tRecall: 0.44366197183098594\n",
      "\tF1: 0.41652892561983473\n",
      "4\n",
      "\tAccuracy: 0.3396584440227704\n",
      "\tPrecision: 0.40804597701149425\n",
      "\tRecall: 0.5\n",
      "\tF1: 0.449367088607595\n",
      "5\n",
      "\tAccuracy: 0.3301707779886148\n",
      "\tPrecision: 0.39184952978056425\n",
      "\tRecall: 0.44014084507042256\n",
      "\tF1: 0.41459369817578773\n",
      "6\n",
      "\tAccuracy: 0.3187855787476281\n",
      "\tPrecision: 0.38095238095238093\n",
      "\tRecall: 0.4225352112676056\n",
      "\tF1: 0.40066777963272115\n",
      "7\n",
      "\tAccuracy: 0.3415559772296015\n",
      "\tPrecision: 0.4036697247706422\n",
      "\tRecall: 0.4647887323943662\n",
      "\tF1: 0.4320785597381342\n",
      "8\n",
      "\tAccuracy: 0.3586337760910816\n",
      "\tPrecision: 0.41916167664670656\n",
      "\tRecall: 0.49295774647887325\n",
      "\tF1: 0.4530744336569579\n",
      "9\n",
      "\tAccuracy: 0.33206831119544594\n",
      "\tPrecision: 0.3994082840236686\n",
      "\tRecall: 0.4753521126760563\n",
      "\tF1: 0.4340836012861736\n",
      "10\n",
      "\tAccuracy: 0.30740037950664134\n",
      "\tPrecision: 0.35789473684210527\n",
      "\tRecall: 0.3591549295774648\n",
      "\tF1: 0.3585237258347979\n"
     ]
    }
   ],
   "source": [
    "for seed in range(1,11):\n",
    "    test_hard_samples(hard_samples, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79d9ead1-b6e4-49f1-9858-0854f72d8611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\tAccuracy: 0.6710526315789473\n",
      "\tPrecision: 0.6376470588235295\n",
      "\tRecall: 0.7923976608187134\n",
      "\tF1: 0.7066492829204694\n",
      "2\n",
      "\tAccuracy: 0.672514619883041\n",
      "\tPrecision: 0.6569148936170213\n",
      "\tRecall: 0.7222222222222222\n",
      "\tF1: 0.6880222841225627\n",
      "3\n",
      "\tAccuracy: 0.6710526315789473\n",
      "\tPrecision: 0.6515544041450777\n",
      "\tRecall: 0.7353801169590644\n",
      "\tF1: 0.6909340659340659\n",
      "4\n",
      "\tAccuracy: 0.6754385964912281\n",
      "\tPrecision: 0.6474201474201474\n",
      "\tRecall: 0.77046783625731\n",
      "\tF1: 0.7036048064085446\n",
      "5\n",
      "\tAccuracy: 0.6608187134502924\n",
      "\tPrecision: 0.6406649616368286\n",
      "\tRecall: 0.7324561403508771\n",
      "\tF1: 0.6834924965893587\n",
      "6\n",
      "\tAccuracy: 0.658625730994152\n",
      "\tPrecision: 0.6422018348623854\n",
      "\tRecall: 0.716374269005848\n",
      "\tF1: 0.6772633033863165\n",
      "7\n",
      "\tAccuracy: 0.6695906432748538\n",
      "\tPrecision: 0.6475826972010178\n",
      "\tRecall: 0.7441520467836257\n",
      "\tF1: 0.6925170068027211\n",
      "8\n",
      "\tAccuracy: 0.6673976608187134\n",
      "\tPrecision: 0.6418835192069393\n",
      "\tRecall: 0.7573099415204678\n",
      "\tF1: 0.6948356807511736\n",
      "9\n",
      "\tAccuracy: 0.6615497076023392\n",
      "\tPrecision: 0.6365883807169345\n",
      "\tRecall: 0.7529239766081871\n",
      "\tF1: 0.6898861352980576\n",
      "10\n",
      "\tAccuracy: 0.6673976608187134\n",
      "\tPrecision: 0.6619519094766619\n",
      "\tRecall: 0.6842105263157895\n",
      "\tF1: 0.6728971962616822\n"
     ]
    }
   ],
   "source": [
    "test_samples = []\n",
    "for convo in corpus.iter_conversations():\n",
    "    if convo.meta['split'] == 'test':\n",
    "        test_samples.append(convo.id)\n",
    "for seed in range(1,11):\n",
    "    test_hard_samples(test_samples, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a11f718-5037-41ec-9240-3b35de3b8f94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(max([0,1,0,0,0,0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "680c046c-32fc-4fd6-91d8-33eeeb784cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dynamic_samples(all_predictions, corpus, corpus_name):\n",
    "    label_metadata = \"conversation_has_personal_attack\" if corpus_name == \"wikiconv\" else \"has_removed_comment\"\n",
    "    num_convo = 0\n",
    "    hard_att = 0\n",
    "    hard_non = 0\n",
    "    dynamic_samples = []\n",
    "    for convo in corpus.iter_conversations():\n",
    "        if convo.meta['split'] == 'test':\n",
    "            max_agreement = 0\n",
    "            for utterance in convo.iter_utterances():\n",
    "                id = utterance.id\n",
    "                if id in all_predictions:\n",
    "                    if all_predictions[id] > max_agreement:\n",
    "                        max_agreement = all_predictions[id]\n",
    "            if convo.meta[label_metadata] == False:\n",
    "                if max_agreement >= 4:\n",
    "                    hard_non += 1\n",
    "                    dynamic_samples.append(convo.id)\n",
    "            else:\n",
    "                if max_agreement <= 6:\n",
    "                    hard_att += 1\n",
    "                    dynamic_samples.append(convo.id)\n",
    "    print(\"We have {} positive samples and {} negative samples\".format(hard_att, hard_non))\n",
    "    return dynamic_samples\n",
    "def test(test_samples, pred_path):\n",
    "    pred_file = open(pred_path, 'r')\n",
    "    pred_lines = pred_file.readlines()[1:]\n",
    "    pred_dict = {}\n",
    "    for line in pred_lines:\n",
    "        id2pred = line.split(\",\")\n",
    "        \n",
    "        assert len(id2pred) == 3\n",
    "        utt_id = id2pred[0]\n",
    "        utt_pred = id2pred[1]\n",
    "        pred_dict[utt_id] = int(utt_pred)\n",
    "        \n",
    "    for convo in corpus.iter_conversations():\n",
    "        # only consider test set conversations (we did not make predictions for the other ones)\n",
    "        if convo.id in test_samples:\n",
    "            for utt in convo.iter_utterances():\n",
    "                if utt.id in pred_dict:\n",
    "                    utt.meta['forecast_score'] = pred_dict[utt.id]\n",
    "    \n",
    "    conversational_forecasts_df = {\n",
    "            \"convo_id\": [],\n",
    "            \"label\": [],\n",
    "            \"prediction\": []\n",
    "        }\n",
    "    for convo in corpus.iter_conversations():\n",
    "        if convo.id in test_samples:\n",
    "            conversational_forecasts_df['convo_id'].append(convo.id)\n",
    "            conversational_forecasts_df['label'].append(int(convo.meta[label_metadata]))\n",
    "            forecast_scores = [utt.meta['forecast_score'] for utt in convo.iter_utterances() if 'forecast_score' in utt.meta]\n",
    "            conversational_forecasts_df['prediction'].append(max(forecast_scores))\n",
    "    conversational_forecasts_df = pd.DataFrame(conversational_forecasts_df).set_index(\"convo_id\")\n",
    "    test_labels = conversational_forecasts_df.label\n",
    "    test_preds = conversational_forecasts_df.prediction\n",
    "    test_acc = (test_labels == test_preds).mean()\n",
    "    \n",
    "    tp = ((test_labels==1)&(test_preds==1)).sum()\n",
    "    fp = ((test_labels==0)&(test_preds==1)).sum()\n",
    "    tn = ((test_labels==0)&(test_preds==0)).sum()\n",
    "    fn = ((test_labels==1)&(test_preds==0)).sum()\n",
    "\n",
    "    test_precision = tp / (tp + fp)\n",
    "    test_recall = tp / (tp + fn)\n",
    "    test_fpr = fp / (fp + tn)\n",
    "    test_f1 = 2 / (((tp + fp) / tp) + ((tp + fn) / tp))\n",
    "    return test_acc, test_precision, test_recall, test_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1bb00c88-2fbc-4438-adf5-7af6e314aeaa",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Corpus' object has no attribute 'help'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mcorpus\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhelp\u001b[49m())\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Corpus' object has no attribute 'help'"
     ]
    }
   ],
   "source": [
    "def get_single_utt_preds(saved_path):\n",
    "    single_utt_predictions = {}\n",
    "    for seed in all_seeds:\n",
    "        pred_path = os.path.join(saved_path, seed, \"predictions.csv\")\n",
    "        pred_file = open(pred_path, 'r')\n",
    "        pred_lines = pred_file.readlines()[1:]\n",
    "        for line in pred_lines:\n",
    "            id2pred = line.split(\",\")\n",
    "            \n",
    "            assert len(id2pred) == 3\n",
    "            utt_id = id2pred[0]\n",
    "            utt_pred = id2pred[1]\n",
    "            if utt_id not in all_predictions:\n",
    "                single_utt_predictions[utt_id] = int(utt_pred)\n",
    "            else:\n",
    "                single_utt_predictions[utt_id] += int(utt_pred)\n",
    "    return single_utt_predictions\n",
    "def full_evaluate(model_path, corpus, corpus_name):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e101abfb-8522-485d-8838-2a96c90ad671",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jacq-zissou-env-3.11",
   "language": "python",
   "name": "jacq-zissou-env-3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
