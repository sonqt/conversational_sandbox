{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93665106-b475-4c74-bd5a-9209619b555f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from convokit import download, Corpus\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afefe02-fe97-49ff-b333-4e37bd54d059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dynamic_samples(all_predictions, corpus, corpus_name):\n",
    "    label_metadata = \"conversation_has_personal_attack\" if corpus_name == \"wikiconv\" else \"has_removed_comment\"\n",
    "    num_convo = 0\n",
    "    hard_att = 0\n",
    "    hard_non = 0\n",
    "    dynamic_samples = []\n",
    "    test_samples = []\n",
    "    for convo in corpus.iter_conversations():\n",
    "        if convo.meta['split'] == 'test':\n",
    "            test_samples.append(convo.id)\n",
    "            max_agreement = 0\n",
    "            for utterance in convo.iter_utterances():\n",
    "                id = utterance.id\n",
    "                if id in all_predictions:\n",
    "                    if all_predictions[id] > max_agreement:\n",
    "                        max_agreement = all_predictions[id]\n",
    "            if convo.meta[label_metadata] == False:\n",
    "                if max_agreement >= 3:\n",
    "                    hard_non += 1\n",
    "                    dynamic_samples.append(convo.id)\n",
    "            else:\n",
    "                if max_agreement <= 7:\n",
    "                    hard_att += 1\n",
    "                    dynamic_samples.append(convo.id)\n",
    "    print(\"We have {} positive samples and {} negative dynamic samples\".format(hard_att, hard_non))\n",
    "    single_samples = [id for id in test_samples if id not in dynamic_samples]\n",
    "    return test_samples, dynamic_samples, single_samples\n",
    "def test(test_samples, pred_path, corpus, corpus_name):\n",
    "    label_metadata = \"conversation_has_personal_attack\" if corpus_name == \"wikiconv\" else \"has_removed_comment\"\n",
    "    pred_file = open(pred_path, 'r')\n",
    "    pred_lines = pred_file.readlines()[1:]\n",
    "    pred_dict = {}\n",
    "    for line in pred_lines:\n",
    "        id2pred = line.split(\",\")\n",
    "        \n",
    "        assert len(id2pred) == 3\n",
    "        utt_id = id2pred[0]\n",
    "        utt_pred = id2pred[1]\n",
    "        pred_dict[utt_id] = int(utt_pred)\n",
    "        \n",
    "    for convo in corpus.iter_conversations():\n",
    "        # only consider test set conversations (we did not make predictions for the other ones)\n",
    "        if convo.id in test_samples:\n",
    "            for utt in convo.iter_utterances():\n",
    "                if utt.id in pred_dict:\n",
    "                    utt.meta['forecast_score'] = pred_dict[utt.id]\n",
    "    \n",
    "    conversational_forecasts_df = {\n",
    "            \"convo_id\": [],\n",
    "            \"label\": [],\n",
    "            \"prediction\": []\n",
    "        }\n",
    "    for convo in corpus.iter_conversations():\n",
    "        if convo.id in test_samples:\n",
    "            conversational_forecasts_df['convo_id'].append(convo.id)\n",
    "            conversational_forecasts_df['label'].append(int(convo.meta[label_metadata]))\n",
    "            forecast_scores = [utt.meta['forecast_score'] for utt in convo.iter_utterances() if 'forecast_score' in utt.meta]\n",
    "            conversational_forecasts_df['prediction'].append(max(forecast_scores))\n",
    "    conversational_forecasts_df = pd.DataFrame(conversational_forecasts_df).set_index(\"convo_id\")\n",
    "    test_labels = conversational_forecasts_df.label\n",
    "    test_preds = conversational_forecasts_df.prediction\n",
    "    test_acc = (test_labels == test_preds).mean()\n",
    "    \n",
    "    tp = ((test_labels==1)&(test_preds==1)).sum()\n",
    "    fp = ((test_labels==0)&(test_preds==1)).sum()\n",
    "    tn = ((test_labels==0)&(test_preds==0)).sum()\n",
    "    fn = ((test_labels==1)&(test_preds==0)).sum()\n",
    "\n",
    "    test_precision = tp / (tp + fp)\n",
    "    test_recall = tp / (tp + fn)\n",
    "    test_fpr = fp / (fp + tn)\n",
    "    test_f1 = 2 / (((tp + fp) / tp) + ((tp + fn) / tp))\n",
    "    return {\"accuracy\":test_acc, \"precision\":test_precision, \"recall\":test_recall, \"f1\":test_f1}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6afc44b-ac55-4451-821a-e0060d75d78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_utt_preds(saved_path):\n",
    "    single_utt_predictions = {}\n",
    "    all_seeds = os.listdir(saved_path)\n",
    "    for seed in all_seeds:\n",
    "        pred_path = os.path.join(saved_path, seed, \"predictions.csv\")\n",
    "        pred_file = open(pred_path, 'r')\n",
    "        pred_lines = pred_file.readlines()[1:]\n",
    "        for line in pred_lines:\n",
    "            id2pred = line.split(\",\")\n",
    "            \n",
    "            assert len(id2pred) == 3\n",
    "            utt_id = id2pred[0]\n",
    "            utt_pred = id2pred[1]\n",
    "            if utt_id not in single_utt_predictions:\n",
    "                single_utt_predictions[utt_id] = int(utt_pred)\n",
    "            else:\n",
    "                single_utt_predictions[utt_id] += int(utt_pred)\n",
    "    return single_utt_predictions\n",
    "def full_evaluate(model_name, full_model_path, single_model_path, corpus, corpus_name):\n",
    "    single_model_path = os.path.join(single_model_path, corpus_name, model_name)\n",
    "    full_model_path = os.path.join(full_model_path, corpus_name, model_name)\n",
    "    \n",
    "    single_utt_predictions = get_single_utt_preds(single_model_path)\n",
    "    test_samples, dynamic_samples, single_samples = extract_dynamic_samples(single_utt_predictions, corpus, corpus_name)\n",
    "\n",
    "    with open('{}.txt'.format(corpus_name), 'w') as f:\n",
    "        for id in dynamic_samples:\n",
    "            f.write(\"%s\\n\" % id)\n",
    "\n",
    "    result_dict = {\"full_test\": {\"accuracy\":[], \"precision\":[], \"recall\":[], \"f1\":[]},\n",
    "                  \"dynamic_only\": {\"accuracy\":[], \"precision\":[], \"recall\":[], \"f1\":[]},\n",
    "                  \"single_enough\": {\"accuracy\":[], \"precision\":[], \"recall\":[], \"f1\":[]}}\n",
    "\n",
    "    for seed in range(1,11):        \n",
    "        pred_path = os.path.join(full_model_path, \"seed-{}\".format(seed), \"predictions.csv\")\n",
    "        full_test = test(test_samples, pred_path, corpus, corpus_name)\n",
    "        for metric in full_test:\n",
    "            result_dict['full_test'][metric].append(full_test[metric])\n",
    "        dynamic_only = test(dynamic_samples, pred_path, corpus, corpus_name)\n",
    "        for metric in dynamic_only:\n",
    "            result_dict['dynamic_only'][metric].append(dynamic_only[metric])\n",
    "        single_enough = test(single_samples, pred_path, corpus, corpus_name)\n",
    "        for metric in single_enough:\n",
    "            result_dict['single_enough'][metric].append(single_enough[metric])\n",
    "    for metric in result_dict['full_test']:\n",
    "        result_dict['full_test'][metric] = np.mean(result_dict['full_test'][metric])\n",
    "        result_dict['dynamic_only'][metric] = np.mean(result_dict['dynamic_only'][metric])\n",
    "        result_dict['single_enough'][metric] = np.mean(result_dict['single_enough'][metric])\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0adcbc5-3aca-4850-84fb-b4837fb75e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model_path = \"/reef/sqt2/BERTCRAFT\"\n",
    "single_model_path = \"/reef/sqt2/SINGLE_UTT\"\n",
    "corpus_name = \"wikiconv\"\n",
    "if corpus_name == \"wikiconv\":\n",
    "    corpus = Corpus(filename=download(\"conversations-gone-awry-corpus\"))\n",
    "elif corpus_name == \"cmv\":\n",
    "    corpus = Corpus(filename=download(\"conversations-gone-awry-cmv-corpus\"))\n",
    "else:\n",
    "    raise Exception(\"Sorry, no corpus_name matched the input {}.\\\n",
    "     Please input a valid corpus_name [wikiconv, cmv]\".format(corpus_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79033af-95e5-40ec-99a3-eb38d6583d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full_evaluate(\"roberta-base\", full_model_path, single_model_path, corpus, corpus_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab8a800-733e-4d4a-8250-ac8958f8c6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance of single_utterance models\n",
    "print(full_evaluate(\"roberta-base\", single_model_path, single_model_path, corpus, corpus_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b26f73e-a7c6-4aec-beeb-d56188fc492d",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model_path = \"/reef/sqt2/BERTCRAFT\"\n",
    "single_model_path = \"/reef/sqt2/SINGLE_UTT\"\n",
    "corpus_name = \"cmv\"\n",
    "if corpus_name == \"wikiconv\":\n",
    "    corpus = Corpus(filename=download(\"conversations-gone-awry-corpus\"))\n",
    "elif corpus_name == \"cmv\":\n",
    "    corpus = Corpus(filename=download(\"conversations-gone-awry-cmv-corpus\"))\n",
    "else:\n",
    "    raise Exception(\"Sorry, no corpus_name matched the input {}.\\\n",
    "     Please input a valid corpus_name [wikiconv, cmv]\".format(corpus_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb62022-58d1-4867-85bc-034c640fbaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full_evaluate(\"roberta-base\", full_model_path, single_model_path, corpus, corpus_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e006c395-45f0-4a15-af80-4b96c6e84883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance of single_utterance models\n",
    "print(full_evaluate(\"roberta-base\", single_model_path, single_model_path, corpus, corpus_name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jacq-zissou-env-3.11",
   "language": "python",
   "name": "jacq-zissou-env-3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
