{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/Users/tran_s2/.local/lib/python3.11/site-packages/huggingface_hub/utils/_runtime.py:184: UserWarning: Pydantic is installed but cannot be imported. Please check your installation. `huggingface_hub` will default to not using Pydantic. Error message: '{e}'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists at /Volumes/Users/tran_s2/.convokit/downloads/conversations-gone-awry-corpus\n"
     ]
    }
   ],
   "source": [
    "from convokit import Corpus, download\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from transformers import RobertaPreTrainedModel\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import RobertaModel\n",
    "\n",
    "corpus = Corpus(filename=download(\"conversations-gone-awry-corpus\"))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"roberta-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadPairs(corpus, split=None, last_only=False):\n",
    "    \"\"\"\n",
    "    Load context-reply pairs from the Corpus, optionally filtering to only conversations\n",
    "    from the specified split (train, val, or test).\n",
    "    Each conversation, which has N comments (not including the section header) will\n",
    "    get converted into N-1 comment-reply pairs, one pair for each reply\n",
    "    (the first comment does not reply to anything).\n",
    "    Each comment-reply pair is a tuple consisting of the conversational context\n",
    "    (that is, all comments prior to the reply), the reply itself, the label (that\n",
    "    is, whether the reply contained a derailment event), and the comment ID of the\n",
    "    reply (for later use in re-joining with the ConvoKit corpus).\n",
    "    The function returns a list of such pairs.\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    count_attack = 0\n",
    "    count_convo = 0\n",
    "    for convo in corpus.iter_conversations():\n",
    "        # consider only conversations in the specified split of the data\n",
    "        if split is None or convo.meta['split'] == split:\n",
    "            count_convo += 1\n",
    "            utterance_list = []\n",
    "            for utterance in convo.iter_utterances():\n",
    "                if utterance.meta['is_section_header']:\n",
    "                    continue\n",
    "                if utterance.meta['comment_has_personal_attack']:\n",
    "                    count_attack += 1\n",
    "                utterance_list.append({\"text\": utterance.text, \n",
    "                                        \"is_attack\": int(utterance.meta['comment_has_personal_attack']), \n",
    "                                        \"id\": utterance.id})\n",
    "                \n",
    "            iter_range = range(1, len(utterance_list)) if not last_only else [len(utterance_list)-1]\n",
    "            for idx in iter_range:\n",
    "                reply = utterance_list[idx][\"text\"]\n",
    "                label = utterance_list[idx][\"is_attack\"]\n",
    "                comment_id = utterance_list[idx][\"id\"]\n",
    "                # gather as context all utterances preceding the reply\n",
    "                context = [u[\"text\"] for u in utterance_list[idx-1:idx]]\n",
    "                pairs.append((context, label))\n",
    "\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pairs = loadPairs(corpus, split='train', last_only=True)\n",
    "val_pairs = loadPairs(corpus, split='val', last_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_len(tokens_list):\n",
    "    index = 0\n",
    "    for i, tokens in enumerate(tokens_list):\n",
    "        if len(tokens) > len(tokens_list[index]):\n",
    "            index = i\n",
    "    return index\n",
    "def tokenize_context(context, tokenizer, max_len=512):\n",
    "    tokenized_context = []\n",
    "    for utterance in context:\n",
    "        tokenized_context.append(tokenizer.encode(utterance, add_special_tokens=False))\n",
    "    def truncate_context(tokenized_context, max_len):\n",
    "        if sum([len(utterance) for utterance in tokenized_context])\\\n",
    "           + len(tokenized_context) + 1 < max_len:\n",
    "            final_context = [tokenizer.cls_token_id]\n",
    "            for utterance in tokenized_context:\n",
    "                final_context += utterance + [tokenizer.sep_token_id]\n",
    "            \n",
    "            padding = [tokenizer.pad_token_id] * (max_len - len(final_context))\n",
    "            mask = [1 for _ in range(len(final_context))] + [0 for _ in range(len(padding))]\n",
    "            return final_context + padding, mask\n",
    "        \n",
    "        while sum([len(utterance) for utterance in tokenized_context])\\\n",
    "              + len(tokenized_context) + 1 > max_len:\n",
    "            truncate_idx = find_max_len(tokenized_context)\n",
    "            tokenized_context[truncate_idx] = tokenized_context[truncate_idx][:-1]\n",
    "        final_context = [tokenizer.cls_token_id]\n",
    "        for utterance in tokenized_context:\n",
    "            final_context += utterance + [tokenizer.sep_token_id]\n",
    "        return final_context, [1 for _ in range(len(final_context))]\n",
    "    return truncate_context(tokenized_context, max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2508\n",
      "840\n"
     ]
    }
   ],
   "source": [
    "print(len(train_pairs))\n",
    "print(len(val_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.5\n"
     ]
    }
   ],
   "source": [
    "count_pos = 0\n",
    "single_comment = 0\n",
    "for i in range(len(train_pairs)):\n",
    "    if train_pairs[i][1] == 1:\n",
    "        count_pos += 1\n",
    "print(single_comment/len(train_pairs), count_pos/len(train_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.5\n"
     ]
    }
   ],
   "source": [
    "count_pos = 0\n",
    "single_comment = 0\n",
    "for i in range(len(val_pairs)):\n",
    "    if val_pairs[i][1] == 1:\n",
    "        count_pos += 1\n",
    "print(single_comment/len(val_pairs), count_pos/len(val_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set our sequence length to pad or truncate all of our samples to.\n",
    "max_len = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "def build_dataloadder(pairs):\n",
    "    question = \"Is this conversation likely to turn into personal attack?\"\n",
    "    input_ids = []\n",
    "    attn_masks = []\n",
    "    labels = []\n",
    "    print('Encoding {:,} training examples...'.format(len(train_pairs)))\n",
    "\n",
    "    # For every training example...\n",
    "    for context, label in pairs:\n",
    "\n",
    "        # Report progress.\n",
    "        if ((len(input_ids) % 500) == 0):\n",
    "            print('  Encoded {:,} comments.'.format(len(input_ids)))\n",
    "\n",
    "        # Convert sentence pairs to input IDs, with attention masks.\n",
    "        # encoded_dict = tokenizer.encode_plus(*context, question,\n",
    "        #                                     max_length=max_len,    # Pad or truncate to this lenght.\n",
    "        #                                     pad_to_max_length=True,\n",
    "        #                                     truncation=True, \n",
    "        #                                     return_tensors='pt')   # Return objects as PyTorch tensors.\n",
    "\n",
    "        # # Add this example to our lists.\n",
    "        # input_ids.append(encoded_dict['input_ids'])\n",
    "        # attn_masks.append(encoded_dict['attention_mask'])\n",
    "        input_id, attn_mask = tokenize_context(context, tokenizer, max_len)\n",
    "        assert len(input_id) == len(attn_mask) == max_len\n",
    "        input_ids.append(torch.tensor(input_id))\n",
    "        attn_masks.append(torch.tensor(attn_mask))\n",
    "        labels.append(label)\n",
    "\n",
    "    # labels = labels.astype(float)\n",
    "    # Cast the labels list to a 2D Tensor.\n",
    "    labels = torch.tensor(labels)\n",
    "    # print(labels.type())\n",
    "    input_ids = torch.stack(input_ids, dim=0)\n",
    "    attn_masks = torch.stack(attn_masks, dim=0)\n",
    "    print(input_ids.shape, attn_masks.shape, labels.shape)\n",
    "    dataset = TensorDataset(input_ids, attn_masks, labels)\n",
    "    print('DONE.')\n",
    "    return DataLoader(dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (752 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding 2,508 training examples...\n",
      "  Encoded 0 comments.\n",
      "  Encoded 500 comments.\n",
      "  Encoded 1,000 comments.\n",
      "  Encoded 1,500 comments.\n",
      "  Encoded 2,000 comments.\n",
      "  Encoded 2,500 comments.\n",
      "torch.Size([2508, 512]) torch.Size([2508, 512]) torch.Size([2508])\n",
      "DONE.\n",
      "Encoding 2,508 training examples...\n",
      "  Encoded 0 comments.\n",
      "  Encoded 500 comments.\n",
      "torch.Size([840, 512]) torch.Size([840, 512]) torch.Size([840])\n",
      "DONE.\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = build_dataloadder(train_pairs)\n",
    "val_dataloader = build_dataloadder(val_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AdamW\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/Users/tran_s2/.local/lib/python3.11/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5,\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs (BERT authors recommend between 2 and 4)\n",
    "epochs = 30\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))  \n",
    "def good_update_interval(total_iters, num_desired_updates):\n",
    "    '''\n",
    "    This function will try to pick an intelligent progress update interval \n",
    "    based on the magnitude of the total iterations.\n",
    "\n",
    "    Parameters:\n",
    "      `total_iters` - The number of iterations in the for-loop.\n",
    "      `num_desired_updates` - How many times we want to see an update over the \n",
    "                              course of the for-loop.\n",
    "    '''\n",
    "    # Divide the total iterations by the desired number of updates. Most likely\n",
    "    # this will be some ugly number.\n",
    "    exact_interval = total_iters / num_desired_updates\n",
    "\n",
    "    # The `round` function has the ability to round down a number to, e.g., the\n",
    "    # nearest thousandth: round(exact_interval, -3)\n",
    "    #\n",
    "    # To determine the magnitude to round to, find the magnitude of the total,\n",
    "    # and then go one magnitude below that.\n",
    "\n",
    "    # Get the order of magnitude of the total.\n",
    "    order_of_mag = len(str(total_iters)) - 1\n",
    "\n",
    "    # Our update interval should be rounded to an order of magnitude smaller. \n",
    "    round_mag = order_of_mag - 1\n",
    "\n",
    "    # Round down and cast to an int.\n",
    "    update_interval = int(round(exact_interval, -round_mag))\n",
    "\n",
    "    # Don't allow the interval to be zero!\n",
    "    if update_interval == 0:\n",
    "        update_interval = 1\n",
    "\n",
    "    return update_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f1_score(labels, preds):\n",
    "    # preds = torch.sigmoid(logits) > 0.5\n",
    "    # Calculating precision, recall, and F1 score using PyTorch\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    TP = ((preds == 1) & (labels == 1)).sum().item()\n",
    "    FP = ((preds == 1) & (labels == 0)).sum().item()\n",
    "    FN = ((preds == 0) & (labels == 1)).sum().item()\n",
    "\n",
    "    precision = TP / (TP + FP) if TP + FP > 0 else 0\n",
    "    recall = TP / (TP + FN) if TP + FN > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return f1\n",
    "def calculate_accuracy_score(labels, preds):\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "\n",
    "    return (labels == preds).sum().item()/labels.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_dataloader):\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_loss = 0\n",
    "\n",
    "    predictions, true_labels = [], []\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in val_dataloader:\n",
    "        \n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
    "        # the `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for backprop (training).\n",
    "        with torch.no_grad():   \n",
    "   \n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "            # values prior to applying an activation function like the softmax.\n",
    "            outputs = model(b_input_ids, attention_mask=b_input_mask, \n",
    "                             labels=b_labels)\n",
    "\n",
    "            loss = outputs[0]\n",
    "            logits = outputs[1]\n",
    "\n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences.\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Store predictions and true labels\n",
    "        predictions.append(logits)\n",
    "        true_labels.append(label_ids)\n",
    "\n",
    "    # Measure validation accuracy...\n",
    "\n",
    "    # Combine the results across all batches. \n",
    "    flat_predictions = np.concatenate(predictions, axis=0)\n",
    "    flat_true_labels = np.concatenate(true_labels, axis=0)\n",
    "\n",
    "    # Calculate the validation accuracy.\n",
    "    val_f1 = calculate_f1_score(flat_true_labels, flat_predictions)\n",
    "    val_acc = calculate_accuracy_score(flat_true_labels, flat_predictions)\n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    print(\"  F1 Score: {0:.2f}\".format(val_f1))\n",
    "    print(\"  Accuracy: {0:.2f}\".format(val_acc))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(val_dataloader)\n",
    "    \n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Validation...\n",
      "  F1 Score: 0.67\n",
      "  Accuracy: 0.50\n",
      "  Validation Loss: 0.70\n",
      "  Validation took: 0:00:07\n",
      "\n",
      "======== Epoch 1 / 30 ========\n",
      "Training...\n",
      "  Batch    60  of    627.    Elapsed: 0:00:07.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# This training code is based on the `run_glue.py` script here:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# We'll store a number of quantities such as training and validation loss, \n",
    "# validation accuracy, and timings.\n",
    "training_stats = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    evaluate(model, val_dataloader)\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Put the model into training mode. Don't be mislead--the call to \n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    model.train()\n",
    "\n",
    "    # Pick an interval on which to print progress updates.\n",
    "    update_interval = good_update_interval(\n",
    "                total_iters = len(train_dataloader), \n",
    "                num_desired_updates = 10\n",
    "            )\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update.\n",
    "        if (step % update_interval) == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
    "        # the `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass. PyTorch doesn't do this automatically because \n",
    "        # accumulating the gradients is \"convenient while training RNNs\". \n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # This call returns the loss (because we provided labels) and the \n",
    "        # \"logits\"--the model outputs prior to activation.\n",
    "        outputs = model(b_input_ids, attention_mask=b_input_mask, \n",
    "                             labels=b_labels)\n",
    "\n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "        \n",
    " \n",
    "    \n",
    "\n",
    "    # # Record all statistics from this epoch.\n",
    "    # training_stats.append(\n",
    "    #     {\n",
    "    #         'epoch': epoch_i + 1,\n",
    "    #         'Training Loss': avg_train_loss,\n",
    "    #         'Valid. Loss': avg_val_loss,\n",
    "    #         'Valid. Accur.': val_acc,\n",
    "    #         'Training Time': training_time,\n",
    "    #         'Validation Time': validation_time\n",
    "    #     }\n",
    "    # )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
