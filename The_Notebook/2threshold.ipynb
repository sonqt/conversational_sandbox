{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_evaluate(args, corpus, tokenized_dataset):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "        saved_model_path: models are saved after each epoch.\n",
    "        tokenized_dataset:\n",
    "            tokenized_dataset['val_for_tuning']\n",
    "            tokenized_dataset['test']\n",
    "    \"\"\"\n",
    "    label_metadata = \"conversation_has_personal_attack\" if args.corpus_name == \"wikiconv\" else \"has_removed_comment\"\n",
    "    utt_label_metadata = \"comment_has_personal_attack\" if args.corpus_name == \"wikiconv\" else None\n",
    "    \n",
    "    # Loop through all saved models to find the best model on val_for_tuning\n",
    "    # Evaluate the best model on test\n",
    "    config = AutoConfig.from_pretrained(args.model_name_or_path)\n",
    "\n",
    "    checkpoints = os.listdir(args.output_dir)\n",
    "    best_val_accuracy = 0\n",
    "    for cp in checkpoints:\n",
    "        full_model_path = os.path.join(args.output_dir, cp)\n",
    "        finetuned_model = AutoModelForSequenceClassification.from_pretrained(full_model_path)\n",
    "        val_scores = evaluateDataset(tokenized_dataset[\"val_for_tuning\"], finetuned_model, \"cuda\")\n",
    "        # for each CONVERSATION, whether or not it triggers will be effectively determined by what the highest score it ever got was\n",
    "        highest_convo_scores = {c.id: -1 for c in corpus.iter_conversations(lambda convo: convo.meta['split']==\"val\")}\n",
    "        for utt_id in val_scores.index:\n",
    "            parent_convo = corpus.get_utterance(utt_id).get_conversation()\n",
    "            utt_score = val_scores.loc[utt_id].score\n",
    "            if utt_score > highest_convo_scores[parent_convo.id]:\n",
    "                highest_convo_scores[parent_convo.id] = utt_score\n",
    "        val_convo_ids = [c.id for c in corpus.iter_conversations(lambda convo: convo.meta['split'] == 'val')]\n",
    "        val_labels = np.asarray([int(corpus.get_conversation(c).meta[label_metadata]) for c in val_convo_ids])\n",
    "        val_scores = np.asarray([highest_convo_scores[c] for c in val_convo_ids])\n",
    "        \n",
    "        # use scikit learn to find candidate threshold cutoffs\n",
    "        _, _, thresholds = roc_curve(val_labels, val_scores)\n",
    "        accs = [acc_with_threshold(val_labels, val_scores, t) for t in thresholds]\n",
    "        \n",
    "        best_acc_idx = np.argmax(accs)\n",
    "        if accs[best_acc_idx] > best_val_accuracy:\n",
    "            best_val_accuracy = accs[best_acc_idx]\n",
    "            best_threshold = thresholds[best_acc_idx]\n",
    "            best_model = finetuned_model\n",
    "    \n",
    "    forecasts_df = evaluateDataset(tokenized_dataset[\"val_for_tuning\"], best_model, \"cuda\", threshold=best_threshold)\n",
    "    prediction_file = os.path.join(args.output_dir, \"pred_val.csv\")\n",
    "    forecasts_df.to_csv(prediction_file)\n",
    "    \n",
    "    forecasts_df = evaluateDataset(tokenized_dataset[\"test\"], best_model, \"cuda\", threshold=best_threshold)\n",
    "    prediction_file = os.path.join(args.output_dir, \"pred_test.csv\")\n",
    "    forecasts_df.to_csv(prediction_file)\n",
    "    # We will add a metadata entry to each test-set utterance signifying whether, at the time\n",
    "    # that CRAFT saw the context *up to and including* that utterance, CRAFT forecasted the\n",
    "    # conversation would derail. Note that in datasets where the actual toxic comment is\n",
    "    # included (such as wikiconv), we explicitly do not show that comment to CRAFT (since\n",
    "    # that would be cheating!), so that comment will not have an associated forecast.\n",
    "    for convo in corpus.iter_conversations():\n",
    "        # only consider test set conversations (we did not make predictions for the other ones)\n",
    "        if convo.meta['split'] == \"test\":\n",
    "            for utt in convo.iter_utterances():\n",
    "                if utt.id in forecasts_df.index:\n",
    "                    utt.meta['forecast_score'] = forecasts_df.loc[utt.id].score\n",
    "    \n",
    "    conversational_forecasts_df = {\n",
    "        \"convo_id\": [],\n",
    "        \"label\": [],\n",
    "        \"score\": [],\n",
    "        \"prediction\": []\n",
    "    }\n",
    "\n",
    "    for convo in corpus.iter_conversations():\n",
    "        if convo.meta['split'] == \"test\":\n",
    "            conversational_forecasts_df['convo_id'].append(convo.id)\n",
    "            conversational_forecasts_df['label'].append(int(convo.meta[label_metadata]))\n",
    "            forecast_scores = [utt.meta['forecast_score'] for utt in convo.iter_utterances() if 'forecast_score' in utt.meta]\n",
    "            conversational_forecasts_df['score'] = np.max(forecast_scores)\n",
    "            conversational_forecasts_df['prediction'].append(int(np.max(forecast_scores) > best_threshold))\n",
    "\n",
    "    conversational_forecasts_df = pd.DataFrame(conversational_forecasts_df).set_index(\"convo_id\")\n",
    "    test_labels = conversational_forecasts_df.label\n",
    "    test_preds = conversational_forecasts_df.prediction\n",
    "    test_acc = (test_labels == test_preds).mean()\n",
    "    \n",
    "\n",
    "    tp = ((test_labels==1)&(test_preds==1)).sum()\n",
    "    fp = ((test_labels==0)&(test_preds==1)).sum()\n",
    "    tn = ((test_labels==0)&(test_preds==0)).sum()\n",
    "    fn = ((test_labels==1)&(test_preds==0)).sum()\n",
    "    test_precision = tp / (tp + fp)\n",
    "    test_recall = tp / (tp + fn)\n",
    "    test_fpr = fp / (fp + tn)\n",
    "    test_f1 = 2 / (((tp + fp) / tp) + ((tp + fn) / tp))\n",
    "\n",
    "    result_dict = {'accuracy': test_acc, \n",
    "                    'precision': test_precision,\n",
    "                    'recall': test_recall,\n",
    "                    'f1': test_f1, \n",
    "                    'false positive rate': test_fpr}\n",
    "    result_file = os.path.join(args.output_dir, \"result.json\")\n",
    "    with open(result_file, 'w') as f:\n",
    "        json.dump(result_dict, f)\n",
    "    return"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
