{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c4c0e8f1-45eb-4e67-89f5-03e963d587c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.8773,  1.2374, -0.3002], requires_grad=True)\n",
      "tensor([1., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "input = torch.randn(3, requires_grad=True)\n",
    "target = torch.empty(3).random_(2)\n",
    "print(input)\n",
    "print(target)\n",
    "# loss = F.binary_cross_entropy_with_logits(input, target)\n",
    "# loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a0243d2-e4c6-4149-9862-6ce6555abfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from convokit import download, Corpus\n",
    "from sklearn.metrics import roc_curve\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01965fbc-a373-4ebf-ab6f-220e60ee53fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists at /home/sqt2/.convokit/downloads/conversations-gone-awry-corpus\n",
      "Dataset already exists at /home/sqt2/.convokit/downloads/conversations-gone-awry-cmv-corpus\n"
     ]
    }
   ],
   "source": [
    "wikicorpus = Corpus(filename=download(\"conversations-gone-awry-corpus\"))\n",
    "cmvcorpus = Corpus(filename=download(\"conversations-gone-awry-cmv-corpus\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f6eff4fb-cefc-4722-a66c-72509d8e02de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_logits(saved_path):\n",
    "    id = []\n",
    "    logit0 = []\n",
    "    logit1 = []\n",
    "    data = {\"id\": [], \"logit0\": [], \"logit1\": []}\n",
    "    pred_file = open(saved_path, 'r')\n",
    "    pred_lines = pred_file.readlines()[1:]\n",
    "    for line in pred_lines:\n",
    "        id2pred = line.split(\",\")\n",
    "        \n",
    "        assert len(id2pred) == 5\n",
    "        id.append(id2pred[0])\n",
    "        logit0.append(id2pred[3])\n",
    "        logit1.append(id2pred[4])\n",
    "    df = pd.DataFrame({\"logit0\": logit0, \"logit1\": logit1}, index=id)\n",
    "    convert_dict = {'logit0': float, 'logit1': float}\n",
    " \n",
    "    df = df.astype(convert_dict)\n",
    "    return df\n",
    "    \n",
    "def calculate_counterfactual(saved_model, split, alpha):\n",
    "    orig_path = os.path.join(saved_model, \"{}_predictions.csv\".format(split))\n",
    "    orig_logits = load_logits(orig_path)\n",
    "\n",
    "    counterfactual_path = os.path.join(saved_model, \"{}_counterfactual_predictions.csv\".format(split))\n",
    "    counterfactual_logits = load_logits(counterfactual_path)\n",
    "    counterfactual_logits[\"logit0\"] = counterfactual_logits[\"logit0\"] * float(alpha)\n",
    "    counterfactual_logits[\"logit1\"] = counterfactual_logits[\"logit1\"] * float(alpha)\n",
    "\n",
    "    final_logits = pd.DataFrame()\n",
    "    final_logits[\"logit0\"] = orig_logits[\"logit0\"] - counterfactual_logits[\"logit0\"]\n",
    "    final_logits[\"logit1\"] = orig_logits[\"logit1\"] - counterfactual_logits[\"logit1\"]\n",
    "    final_logits[\"score\"] = softmax(final_logits[['logit0', 'logit1']].to_numpy(), axis=1)[:,1]\n",
    "    return final_logits\n",
    "    \n",
    "def acc_with_threshold(y_true, y_score, thresh):\n",
    "    y_pred = (y_score > thresh).astype(int)\n",
    "    return (y_pred == y_true).mean() \n",
    "\n",
    "def tune_model_for_val(corpus, corpus_name, saved_model):\n",
    "    label_metadata = \"conversation_has_personal_attack\" if corpus_name == \"wikiconv\" else \"has_removed_comment\"\n",
    "    utt_label_metadata = \"comment_has_personal_attack\" if corpus_name == \"wikiconv\" else None\n",
    "    best_acc = 0\n",
    "    best_alpha = 0\n",
    "    best_threshold = 0\n",
    "    for alpha in np.arange(1.5,1.51,0.1):\n",
    "        val_scores = calculate_counterfactual(saved_model, \"val\", alpha)\n",
    "        highest_convo_scores = {c.id: -1 for c in corpus.iter_conversations(lambda convo: convo.meta[\"split\"] == \"val\")}\n",
    "        for utt_id in val_scores.index:\n",
    "            parent_convo = corpus.get_utterance(utt_id).get_conversation()\n",
    "            utt_score = val_scores.loc[utt_id].score\n",
    "            if parent_convo.id in highest_convo_scores:\n",
    "                if utt_score > highest_convo_scores[parent_convo.id]:\n",
    "                    highest_convo_scores[parent_convo.id] = utt_score\n",
    "        val_convo_ids = [c.id for c in corpus.iter_conversations(lambda convo: convo.meta[\"split\"] == \"val\")]\n",
    "        val_labels = np.asarray([int(corpus.get_conversation(c).meta[label_metadata]) for c in val_convo_ids])\n",
    "        val_scores = np.asarray([highest_convo_scores[c] for c in val_convo_ids])\n",
    "        _, _, thresholds = roc_curve(val_labels, val_scores)\n",
    "        accs = [acc_with_threshold(val_labels, val_scores, t) for t in thresholds]\n",
    "        best_acc_idx = np.argmax(accs)\n",
    "        # print(\"{} {} |||| Achieved Accuracy:\".format(alpha, thresholds[best_acc_idx]), accs[best_acc_idx])\n",
    "        if accs[best_acc_idx] > best_acc:\n",
    "            best_acc = accs[best_acc_idx]\n",
    "            best_alpha = alpha\n",
    "            best_threshold = thresholds[best_acc_idx]\n",
    "    print(\"{} {} |||| Achieved Accuracy:\".format(best_alpha, best_threshold), best_acc)\n",
    "    return best_acc, best_alpha, best_threshold\n",
    "\n",
    "def tune_model_for_dynamic(corpus, corpus_name, saved_model):\n",
    "    label_metadata = \"conversation_has_personal_attack\" if corpus_name == \"wikiconv\" else \"has_removed_comment\"\n",
    "    utt_label_metadata = \"comment_has_personal_attack\" if corpus_name == \"wikiconv\" else None\n",
    "    best_acc = 0\n",
    "    best_alpha = 0\n",
    "    best_threshold = 0\n",
    "    for alpha in np.arange(1.5,1.51,0.1):\n",
    "        val_scores = calculate_counterfactual(saved_model, \"val\", alpha)\n",
    "        highest_convo_scores = {c.id: -1 for c in corpus.iter_conversations(lambda convo: convo.meta[\"split\"] == \"val\")}\n",
    "        for utt_id in val_scores.index:\n",
    "            parent_convo = corpus.get_utterance(utt_id).get_conversation()\n",
    "            utt_score = val_scores.loc[utt_id].score\n",
    "            if parent_convo.id in highest_convo_scores:\n",
    "                if utt_score > highest_convo_scores[parent_convo.id]:\n",
    "                    highest_convo_scores[parent_convo.id] = utt_score\n",
    "        val_convo_ids = [c.id for c in corpus.iter_conversations(lambda convo: convo.meta[\"split\"] == \"val\")]\n",
    "        val_labels = np.asarray([int(corpus.get_conversation(c).meta[label_metadata]) for c in val_convo_ids])\n",
    "        val_scores = np.asarray([highest_convo_scores[c] for c in val_convo_ids])\n",
    "        _, _, thresholds = roc_curve(val_labels, val_scores)\n",
    "        accs = [acc_with_threshold(val_labels, val_scores, t) for t in thresholds]\n",
    "        best_acc_idx = np.argmax(accs)\n",
    "        # print(\"{} {} |||| Achieved Accuracy:\".format(alpha, thresholds[best_acc_idx]), accs[best_acc_idx])\n",
    "        if accs[best_acc_idx] > best_acc:\n",
    "            best_acc = accs[best_acc_idx]\n",
    "            best_alpha = alpha\n",
    "            best_threshold = thresholds[best_acc_idx]\n",
    "    print(\"{} {} |||| Achieved Accuracy:\".format(best_alpha, best_threshold), best_acc)\n",
    "    return best_acc, best_alpha, best_threshold\n",
    "    \n",
    "def counterfactual_evaluate(corpus, corpus_name, saved_model):\n",
    "    label_metadata = \"conversation_has_personal_attack\" if corpus_name == \"wikiconv\" else \"has_removed_comment\"\n",
    "    utt_label_metadata = \"comment_has_personal_attack\" if corpus_name == \"wikiconv\" else None\n",
    "    \n",
    "    _, best_alpha, best_threshold = tune_model_for_val(corpus, corpus_name, saved_model)\n",
    "    test_scores = calculate_counterfactual(saved_model, \"test\", best_alpha)\n",
    "    \n",
    "    test_scores[\"prediction\"] = (test_scores[\"score\"] > best_threshold).astype(int)\n",
    "    prediction_file = os.path.join(saved_model, \"counterfactual_final.csv\")\n",
    "    test_scores.to_csv(prediction_file)\n",
    "    \n",
    "    highest_convo_scores = {c.id: -1 for c in corpus.iter_conversations(lambda convo: convo.meta['split']==\"test\")}\n",
    "    for utt_id in test_scores.index:\n",
    "        parent_convo = corpus.get_utterance(utt_id).get_conversation()\n",
    "        utt_score = test_scores.loc[utt_id].score\n",
    "        if utt_score > highest_convo_scores[parent_convo.id]:\n",
    "            highest_convo_scores[parent_convo.id] = utt_score\n",
    "    test_convo_ids = [c.id for c in corpus.iter_conversations(lambda convo: convo.meta['split'] == 'test')]\n",
    "    test_labels = np.asarray([int(corpus.get_conversation(c).meta[label_metadata]) for c in test_convo_ids])\n",
    "    test_scores = np.asarray([highest_convo_scores[c] for c in test_convo_ids])\n",
    "    test_pred = (test_scores > best_threshold).astype(int)\n",
    "    print((test_pred == test_labels).mean())\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9b89f55f-1332-4e35-90b0-c163cb151a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5 0.8211821152695813 |||| Achieved Accuracy: 0.6214285714285714\n",
      "0.6035714285714285\n"
     ]
    }
   ],
   "source": [
    "saved_model = \"/reef/sqt2/BERTCRAFT_counterfactual/wikiconv/roberta-base/seed-1\"\n",
    "counterfactual_evaluate(wikicorpus, \"wikiconv\", saved_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "584bd653-f4b8-4979-8fb8-4d23bbe05c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5 0.801236981197876 |||| Achieved Accuracy: 0.6261904761904762\n",
      "0.6095238095238096\n"
     ]
    }
   ],
   "source": [
    "saved_model = \"/reef/sqt2/BERTCRAFT_counterfactual/wikiconv/roberta-base/seed-2\"\n",
    "counterfactual_evaluate(wikicorpus, \"wikiconv\", saved_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5f2e6f67-fb0a-4180-8916-e8739a4d42af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5 0.7643885192192834 |||| Achieved Accuracy: 0.5845238095238096\n",
      "0.6035714285714285\n"
     ]
    }
   ],
   "source": [
    "saved_model = \"/reef/sqt2/BERTCRAFT_counterfactual/wikiconv/roberta-base/seed-3\"\n",
    "counterfactual_evaluate(wikicorpus, \"wikiconv\", saved_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2e5d1dee-0c8e-41ce-b3d1-88f9372ef5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5 0.6740890164667562 |||| Achieved Accuracy: 0.6047619047619047\n",
      "0.5964285714285714\n"
     ]
    }
   ],
   "source": [
    "saved_model = \"/reef/sqt2/BERTCRAFT_counterfactual/wikiconv/roberta-base/seed-4\"\n",
    "counterfactual_evaluate(wikicorpus, \"wikiconv\", saved_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb2ef47-50a9-4baa-a7f8-05e0c0326bcc",
   "metadata": {},
   "source": [
    "## CMV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3779526a-44b5-43f8-b493-803ec676536e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.6330996735001478 |||| Achieved Accuracy: 0.6769005847953217\n",
      "0.6652046783625731\n"
     ]
    }
   ],
   "source": [
    "saved_model = \"/reef/sqt2/BERTCRAFT_counterfactual/cmv/roberta-base/seed-1\"\n",
    "counterfactual_evaluate(cmvcorpus, \"cmv\", saved_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0ad37a6e-7f78-4b8f-b82c-be3e93dcb3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.4992547301506855 |||| Achieved Accuracy: 0.6732456140350878\n",
      "0.6622807017543859\n"
     ]
    }
   ],
   "source": [
    "saved_model = \"/reef/sqt2/BERTCRAFT_counterfactual/cmv/roberta-base/seed-2\"\n",
    "counterfactual_evaluate(cmvcorpus, \"cmv\", saved_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c9196589-cf79-4863-bb73-bafe505c7d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.328870954988892 |||| Achieved Accuracy: 0.6776315789473685\n",
      "0.6622807017543859\n"
     ]
    }
   ],
   "source": [
    "saved_model = \"/reef/sqt2/BERTCRAFT_counterfactual/cmv/roberta-base/seed-3\"\n",
    "counterfactual_evaluate(cmvcorpus, \"cmv\", saved_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "bb685a76-519d-4edb-a275-f48d58483fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.43241085007452523 |||| Achieved Accuracy: 0.679093567251462\n",
      "0.6754385964912281\n"
     ]
    }
   ],
   "source": [
    "saved_model = \"/reef/sqt2/BERTCRAFT_counterfactual/cmv/roberta-base/seed-4\"\n",
    "counterfactual_evaluate(cmvcorpus, \"cmv\", saved_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265f1683-44f9-4bac-951c-8f429256972a",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe6a530c-13f7-4fab-b178-60ba2f974c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dynamic_samples(all_predictions, corpus, corpus_name):\n",
    "    label_metadata = \"conversation_has_personal_attack\" if corpus_name == \"wikiconv\" else \"has_removed_comment\"\n",
    "    num_convo = 0\n",
    "    hard_pos, hard_neg = 0, 0\n",
    "    all_pos, all_neg = 0, 0\n",
    "    dynamic_samples = []\n",
    "    test_samples = []\n",
    "    for convo in corpus.iter_conversations():\n",
    "        if convo.meta['split'] == 'test':\n",
    "            test_samples.append(convo.id)\n",
    "            max_agreement = 0\n",
    "            for utterance in convo.iter_utterances():\n",
    "                id = utterance.id\n",
    "                if id in all_predictions:\n",
    "                    if all_predictions[id] > max_agreement:\n",
    "                        max_agreement = all_predictions[id]\n",
    "            if convo.meta[label_metadata] == False:\n",
    "                all_neg += 1\n",
    "                if max_agreement >= 3:\n",
    "                    hard_neg += 1\n",
    "                    dynamic_samples.append(convo.id)\n",
    "            else:\n",
    "                all_pos += 1\n",
    "                if max_agreement <= 8:\n",
    "                    hard_pos += 1\n",
    "                    dynamic_samples.append(convo.id)\n",
    "    print(\"We have {} positive samples and {} negative samples in the test set\".format(all_pos, all_neg))\n",
    "    # print(\"We have {} positive samples and {} negative dynamic samples\".format(hard_pos, hard_neg))\n",
    "\n",
    "    single_samples = [id for id in test_samples if id not in dynamic_samples]\n",
    "    print(len(dynamic_samples))\n",
    "    print(len(single_samples))\n",
    "    return test_samples, dynamic_samples, single_samples\n",
    "def test(test_samples, pred_path, corpus, corpus_name):\n",
    "    print(pred_path)\n",
    "    label_metadata = \"conversation_has_personal_attack\" if corpus_name == \"wikiconv\" else \"has_removed_comment\"\n",
    "    pred_file = open(pred_path, 'r')\n",
    "    pred_lines = pred_file.readlines()[1:]\n",
    "    pred_dict = {}\n",
    "    for line in pred_lines:\n",
    "        id2pred = line.split(\",\")\n",
    "        \n",
    "        assert len(id2pred) == 5\n",
    "        utt_id = id2pred[0]\n",
    "        utt_pred = id2pred[4]\n",
    "        pred_dict[utt_id] = int(utt_pred)\n",
    "        \n",
    "    for convo in corpus.iter_conversations():\n",
    "        # only consider test set conversations (we did not make predictions for the other ones)\n",
    "        if convo.id in test_samples:\n",
    "            for utt in convo.iter_utterances():\n",
    "                if utt.id in pred_dict:\n",
    "                    utt.meta['forecast_score'] = pred_dict[utt.id]\n",
    "    \n",
    "    conversational_forecasts_df = {\n",
    "            \"convo_id\": [],\n",
    "            \"label\": [],\n",
    "            \"prediction\": []\n",
    "        }\n",
    "    for convo in corpus.iter_conversations():\n",
    "        if convo.id in test_samples:\n",
    "            conversational_forecasts_df['convo_id'].append(convo.id)\n",
    "            conversational_forecasts_df['label'].append(int(convo.meta[label_metadata]))\n",
    "            forecast_scores = [utt.meta['forecast_score'] for utt in convo.iter_utterances() if 'forecast_score' in utt.meta]\n",
    "            conversational_forecasts_df['prediction'].append(max(forecast_scores))\n",
    "    conversational_forecasts_df = pd.DataFrame(conversational_forecasts_df).set_index(\"convo_id\")\n",
    "    test_labels = conversational_forecasts_df.label\n",
    "    test_preds = conversational_forecasts_df.prediction\n",
    "    test_acc = (test_labels == test_preds).mean()\n",
    "    \n",
    "    tp = ((test_labels==1)&(test_preds==1)).sum()\n",
    "    fp = ((test_labels==0)&(test_preds==1)).sum()\n",
    "    tn = ((test_labels==0)&(test_preds==0)).sum()\n",
    "    fn = ((test_labels==1)&(test_preds==0)).sum()\n",
    "\n",
    "    test_precision = tp / (tp + fp)\n",
    "    test_recall = tp / (tp + fn)\n",
    "    test_fpr = fp / (fp + tn)\n",
    "    test_f1 = 2 / (((tp + fp) / tp) + ((tp + fn) / tp))\n",
    "    return {\"accuracy\":test_acc, \"precision\":test_precision, \"recall\":test_recall, \"f1\":test_f1}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f121760-a682-4cdc-a7ea-31075b39c84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_utt_preds(saved_path):\n",
    "    single_utt_predictions = {}\n",
    "    all_seeds = os.listdir(saved_path)\n",
    "    for seed in all_seeds:\n",
    "        pred_path = os.path.join(saved_path, seed, \"predictions.csv\")\n",
    "        pred_file = open(pred_path, 'r')\n",
    "        pred_lines = pred_file.readlines()[1:]\n",
    "        for line in pred_lines:\n",
    "            id2pred = line.split(\",\")\n",
    "            \n",
    "            assert len(id2pred) == 3\n",
    "            utt_id = id2pred[0]\n",
    "            utt_pred = id2pred[1]\n",
    "            if utt_id not in single_utt_predictions:\n",
    "                single_utt_predictions[utt_id] = int(utt_pred)\n",
    "            else:\n",
    "                single_utt_predictions[utt_id] += int(utt_pred)\n",
    "    return single_utt_predictions\n",
    "    \n",
    "def full_evaluate(full_model_name, full_model_path, single_model_name, single_model_path, corpus, corpus_name):\n",
    "    single_model_path = os.path.join(single_model_path, corpus_name, single_model_name)\n",
    "    full_model_path = os.path.join(full_model_path, corpus_name, full_model_name)\n",
    "    \n",
    "    single_utt_predictions = get_single_utt_preds(single_model_path)\n",
    "    test_samples, dynamic_samples, single_samples = extract_dynamic_samples(single_utt_predictions, corpus, corpus_name)\n",
    "\n",
    "    with open('{}.txt'.format(corpus_name), 'w') as f:\n",
    "        for id in dynamic_samples:\n",
    "            f.write(\"%s\\n\" % id)\n",
    "\n",
    "    result_dict = {\"full_test\": {\"accuracy\":[], \"precision\":[], \"recall\":[], \"f1\":[]},\n",
    "                  \"dynamic_only\": {\"accuracy\":[], \"precision\":[], \"recall\":[], \"f1\":[]},\n",
    "                  \"single_enough\": {\"accuracy\":[], \"precision\":[], \"recall\":[], \"f1\":[]}}\n",
    "\n",
    "    for seed in range(1,5):        \n",
    "        try:\n",
    "            pred_path = os.path.join(full_model_path, \"seed-{}\".format(seed), \"counterfactual_final.csv\")\n",
    "            full_test = test(test_samples, pred_path, corpus, corpus_name)\n",
    "            for metric in full_test:\n",
    "                result_dict['full_test'][metric].append(full_test[metric])\n",
    "            dynamic_only = test(dynamic_samples, pred_path, corpus, corpus_name)\n",
    "            for metric in dynamic_only:\n",
    "                result_dict['dynamic_only'][metric].append(dynamic_only[metric])\n",
    "            single_enough = test(single_samples, pred_path, corpus, corpus_name)\n",
    "            for metric in single_enough:\n",
    "                result_dict['single_enough'][metric].append(single_enough[metric])\n",
    "        except:\n",
    "            continue\n",
    "    for metric in result_dict['full_test']:\n",
    "        result_dict['full_test'][metric] = np.mean(result_dict['full_test'][metric])\n",
    "        result_dict['dynamic_only'][metric] = np.mean(result_dict['dynamic_only'][metric])\n",
    "        result_dict['single_enough'][metric] = np.mean(result_dict['single_enough'][metric])\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70911c77-9d44-45b3-bfa2-9a7d8446c6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_model_path = \"/reef/sqt2/SINGLE_UTT\"\n",
    "full_model_path = \"/reef/sqt2/BERTCRAFT_counterfactual\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3a8f79e-60c1-4884-8b06-56057793b88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 420 positive samples and 420 negative samples in the test set\n",
      "420\n",
      "420\n",
      "/reef/sqt2/BERTCRAFT_counterfactual/wikiconv/roberta-base/seed-1/counterfactual_final.csv\n",
      "/reef/sqt2/BERTCRAFT_counterfactual/wikiconv/roberta-base/seed-1/counterfactual_final.csv\n",
      "/reef/sqt2/BERTCRAFT_counterfactual/wikiconv/roberta-base/seed-1/counterfactual_final.csv\n",
      "/reef/sqt2/BERTCRAFT_counterfactual/wikiconv/roberta-base/seed-2/counterfactual_final.csv\n",
      "/reef/sqt2/BERTCRAFT_counterfactual/wikiconv/roberta-base/seed-2/counterfactual_final.csv\n",
      "/reef/sqt2/BERTCRAFT_counterfactual/wikiconv/roberta-base/seed-2/counterfactual_final.csv\n",
      "/reef/sqt2/BERTCRAFT_counterfactual/wikiconv/roberta-base/seed-3/counterfactual_final.csv\n",
      "/reef/sqt2/BERTCRAFT_counterfactual/wikiconv/roberta-base/seed-3/counterfactual_final.csv\n",
      "/reef/sqt2/BERTCRAFT_counterfactual/wikiconv/roberta-base/seed-3/counterfactual_final.csv\n",
      "/reef/sqt2/BERTCRAFT_counterfactual/wikiconv/roberta-base/seed-4/counterfactual_final.csv\n",
      "/reef/sqt2/BERTCRAFT_counterfactual/wikiconv/roberta-base/seed-4/counterfactual_final.csv\n",
      "/reef/sqt2/BERTCRAFT_counterfactual/wikiconv/roberta-base/seed-4/counterfactual_final.csv\n",
      "{'full_test': {'accuracy': 0.5270833333333333, 'precision': 0.5478696218522392, 'recall': 0.3755952380952381, 'f1': 0.4192917108670164}, 'dynamic_only': {'accuracy': 0.46726190476190477, 'precision': 0.44124766674333304, 'recall': 0.32943925233644855, 'f1': 0.3555902250417704}, 'single_enough': {'accuracy': 0.5869047619047619, 'precision': 0.6687312551872688, 'recall': 0.42354368932038833, 'f1': 0.48892491668623583}}\n"
     ]
    }
   ],
   "source": [
    "print(full_evaluate(\"roberta-base\", full_model_path, \"roberta-base\", single_model_path, wikicorpus, 'wikiconv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "685f7293-07e5-4940-9b83-33d85d3d9fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 684 positive samples and 684 negative samples in the test set\n",
      "643\n",
      "725\n",
      "/reef/sqt2/BERTCRAFT_counterfactual/cmv/roberta-base/seed-1/counterfactual_final.csv\n",
      "/reef/sqt2/BERTCRAFT_counterfactual/cmv/roberta-base/seed-1/counterfactual_final.csv\n",
      "/reef/sqt2/BERTCRAFT_counterfactual/cmv/roberta-base/seed-1/counterfactual_final.csv\n",
      "/reef/sqt2/BERTCRAFT_counterfactual/cmv/roberta-base/seed-2/counterfactual_final.csv\n",
      "/reef/sqt2/BERTCRAFT_counterfactual/cmv/roberta-base/seed-2/counterfactual_final.csv\n",
      "/reef/sqt2/BERTCRAFT_counterfactual/cmv/roberta-base/seed-2/counterfactual_final.csv\n",
      "/reef/sqt2/BERTCRAFT_counterfactual/cmv/roberta-base/seed-3/counterfactual_final.csv\n",
      "/reef/sqt2/BERTCRAFT_counterfactual/cmv/roberta-base/seed-3/counterfactual_final.csv\n",
      "/reef/sqt2/BERTCRAFT_counterfactual/cmv/roberta-base/seed-3/counterfactual_final.csv\n",
      "/reef/sqt2/BERTCRAFT_counterfactual/cmv/roberta-base/seed-4/counterfactual_final.csv\n",
      "/reef/sqt2/BERTCRAFT_counterfactual/cmv/roberta-base/seed-4/counterfactual_final.csv\n",
      "/reef/sqt2/BERTCRAFT_counterfactual/cmv/roberta-base/seed-4/counterfactual_final.csv\n",
      "{'full_test': {'accuracy': 0.6663011695906432, 'precision': 0.64520218216768, 'recall': 0.7437865497076024, 'f1': 0.689526319826724}, 'dynamic_only': {'accuracy': 0.4105754276827372, 'precision': 0.48311612143201516, 'recall': 0.547683923705722, 'f1': 0.5119095749952028}, 'single_enough': {'accuracy': 0.8931034482758621, 'precision': 0.8216889897322677, 'recall': 0.970820189274448, 'f1': 0.8888516134821489}}\n"
     ]
    }
   ],
   "source": [
    "print(full_evaluate(\"roberta-base\", full_model_path, \"roberta-base\", single_model_path, cmvcorpus, 'cmv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b81083ae-916d-4365-9082-25a7c8371da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_model_path = \"/reef/sqt2/SINGLE_UTT\"\n",
    "full_model_path = \"/reef/sqt2/BERTCRAFT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "72e0382b-9f15-46ff-8792-c2dc09debbf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 684 positive samples and 684 negative samples in the test set\n",
      "643\n",
      "725\n",
      "/reef/sqt2/BERTCRAFT/cmv/roberta-base/seed-1/counterfactual_final.csv\n",
      "/reef/sqt2/BERTCRAFT/cmv/roberta-base/seed-2/counterfactual_final.csv\n",
      "/reef/sqt2/BERTCRAFT/cmv/roberta-base/seed-3/counterfactual_final.csv\n",
      "/reef/sqt2/BERTCRAFT/cmv/roberta-base/seed-4/counterfactual_final.csv\n",
      "{'full_test': {'accuracy': nan, 'precision': nan, 'recall': nan, 'f1': nan}, 'dynamic_only': {'accuracy': nan, 'precision': nan, 'recall': nan, 'f1': nan}, 'single_enough': {'accuracy': nan, 'precision': nan, 'recall': nan, 'f1': nan}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/reef/conda-envs/jacq-zissou-env-3.11/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/reef/conda-envs/jacq-zissou-env-3.11/lib/python3.11/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "print(full_evaluate(\"roberta-base\", full_model_path, \"roberta-base\", single_model_path, cmvcorpus, 'cmv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceec1bfa-37e0-4783-89e0-6347aaeef921",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jacq-zissou-env-3.11",
   "language": "python",
   "name": "jacq-zissou-env-3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
