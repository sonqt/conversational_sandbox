{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists at /Volumes/Users/tran_s2/.convokit/downloads/conversations-gone-awry-corpus\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from convokit import Corpus, download\n",
    "corpus = Corpus(filename=download(\"conversations-gone-awry-corpus\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4188\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus.get_conversation_ids()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadPairs(corpus, split=None, last_only=False):\n",
    "    \"\"\"\n",
    "    Load context-reply pairs from the Corpus, optionally filtering to only conversations\n",
    "    from the specified split (train, val, or test).\n",
    "    Each conversation, which has N comments (not including the section header) will\n",
    "    get converted into N-1 comment-reply pairs, one pair for each reply\n",
    "    (the first comment does not reply to anything).\n",
    "    Each comment-reply pair is a tuple consisting of the conversational context\n",
    "    (that is, all comments prior to the reply), the reply itself, the label (that\n",
    "    is, whether the reply contained a derailment event), and the comment ID of the\n",
    "    reply (for later use in re-joining with the ConvoKit corpus).\n",
    "    The function returns a list of such pairs.\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    count_attack = 0\n",
    "    count_convo = 0\n",
    "    for convo in corpus.iter_conversations():\n",
    "        # consider only conversations in the specified split of the data\n",
    "        if split is None or convo.meta['split'] == split:\n",
    "            count_convo += 1\n",
    "            utterance_list = []\n",
    "            for utterance in convo.iter_utterances():\n",
    "                if utterance.meta['is_section_header']:\n",
    "                    continue\n",
    "                if utterance.meta['comment_has_personal_attack']:\n",
    "                    count_attack += 1\n",
    "                utterance_list.append({\"text\": utterance.text, \n",
    "                                        \"is_attack\": int(utterance.meta['comment_has_personal_attack']), \n",
    "                                        \"id\": utterance.id})\n",
    "                \n",
    "            iter_range = range(1, len(utterance_list)) if not last_only else [len(utterance_list)-1]\n",
    "            for idx in iter_range:\n",
    "                reply = utterance_list[idx][\"text\"]\n",
    "                label = utterance_list[idx][\"is_attack\"]\n",
    "                comment_id = utterance_list[idx][\"id\"]\n",
    "                # gather as context all utterances preceding the reply\n",
    "                context = [u[\"text\"] for u in utterance_list[:idx]]\n",
    "                pairs.append((context, reply, label, comment_id))\n",
    "\n",
    "    return pairs\n",
    "def conversations2utterances(conversations):\n",
    "    \"\"\"\n",
    "    Convert list of conversations into list of utterances for UtteranceModel.\n",
    "    INPUT:\n",
    "        conversations: list of list of str\n",
    "            List of conversations, each conversation is a list of utterances.\n",
    "    OUTPUT:\n",
    "        utterances: list of str\n",
    "            List of utterances in the dataset.\n",
    "        conversationLength: list of int\n",
    "            List of number of utterances in conversations.\n",
    "    \"\"\"\n",
    "    conversationLength = [len(convo) for convo in conversations]\n",
    "    utterances = []\n",
    "    for convo in conversations:\n",
    "        for utterance in convo:\n",
    "            utterances.append(utterance)\n",
    "    # assert len(utterances) == sum(conversationLength)\n",
    "    return utterances, conversationLength\n",
    "\n",
    "def load_data(corpus, context_batch_size = 32, split=None, last_only=False, shuffle=True):\n",
    "    \"\"\"\n",
    "    Load data from corpus into the format ready for UtteranceModel.\n",
    "    INPUT:\n",
    "        corpus: convokit.Corpus\n",
    "        split: str, optional\n",
    "            If specified, only consider conversations in the specified split of the data.\n",
    "        last_only: bool, optional\n",
    "            If True, only consider the last utterance in each conversation.\n",
    "    OUTPUT:\n",
    "        utterances: list of str\n",
    "            List of utterances in the dataset.\n",
    "        conversationLength: list of int\n",
    "            List of lengths of conversations in the dataset.\n",
    "        comment_ids: list of str\n",
    "            List of ids corresponding to the reply utterance.\n",
    "        labels: list of int\n",
    "            List of labels for each context if the next reply contains personal attack.\n",
    "    \"\"\"\n",
    "    pairs = loadPairs(corpus, split, last_only)\n",
    "    if shuffle:\n",
    "        random.shuffle(pairs)\n",
    "    batch_labels = []\n",
    "    batch_comment_ids = []\n",
    "    batch_utterances = []\n",
    "    batch_conversationLength = []\n",
    "    conversations = []\n",
    "    labels = []\n",
    "    comment_ids = []\n",
    "    for pair in pairs:\n",
    "        if len(labels) == context_batch_size:\n",
    "            utterances, conversationLength = conversations2utterances(conversations)\n",
    "            batch_utterances.append(utterances)\n",
    "            batch_conversationLength.append(conversationLength)\n",
    "            batch_labels.append(labels)\n",
    "            batch_comment_ids.append(comment_ids)\n",
    "            assert len(conversationLength) == len(comment_ids) == len(labels)\n",
    "            conversations = []\n",
    "            labels = []\n",
    "            comment_ids = []\n",
    "\n",
    "        context, _, label, comment_id = pair\n",
    "        conversations.append(context)\n",
    "        labels.append(label)\n",
    "        comment_ids.append(comment_id)\n",
    "    if len(conversations) > 0:\n",
    "        utterances, conversationLength = conversations2utterances(conversations)\n",
    "        batch_utterances.append(utterances)\n",
    "        batch_conversationLength.append(conversationLength)\n",
    "        batch_labels.append(labels)\n",
    "        batch_comment_ids.append(comment_ids)\n",
    "    return batch_utterances, batch_conversationLength, batch_comment_ids, batch_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UtteranceRNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "class Voc:\n",
    "    \"\"\"A class for representing the vocabulary used by a CRAFT model\"\"\"\n",
    "\n",
    "    def __init__(self, name, word2index=None, index2word=None):\n",
    "        # Default word tokens\n",
    "        self.PAD_token = 0  # Used for padding short sentences\n",
    "        self.SOS_token = 1  # Start-of-sentence token\n",
    "        self.EOS_token = 2  # End-of-sentence token\n",
    "        self.UNK_token = 3  # Unknown word token\n",
    "\n",
    "        self.name = name\n",
    "        self.trimmed = False if not word2index else True # if a precomputed vocab is specified assume the user wants to use it as-is\n",
    "        self.word2index = word2index if word2index else {\"UNK\": self.UNK_token}\n",
    "        self.word2count = {}\n",
    "        self.index2word = index2word if index2word else {self.PAD_token: \"PAD\", self.SOS_token: \"SOS\", self.EOS_token: \"EOS\", self.UNK_token: \"UNK\"}\n",
    "        self.num_words = 4 if not index2word else len(index2word)  # Count SOS, EOS, PAD, UNK\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "    # Remove words below a certain count threshold\n",
    "    def trim(self, min_count):\n",
    "        if self.trimmed:\n",
    "            return\n",
    "        self.trimmed = True\n",
    "\n",
    "        keep_words = []\n",
    "\n",
    "        for k, v in self.word2count.items():\n",
    "            if v >= min_count:\n",
    "                keep_words.append(k)\n",
    "\n",
    "        print('keep_words {} / {} = {:.4f}'.format(\n",
    "            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n",
    "        ))\n",
    "\n",
    "        # Reinitialize dictionaries\n",
    "        self.word2index = {\"UNK\": self.UNK_token}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {self.PAD_token: \"PAD\", self.SOS_token: \"SOS\", self.EOS_token: \"EOS\", self.UNK_token: \"UNK\"}\n",
    "        self.num_words = 4 # Count default tokens\n",
    "        for word in keep_words:\n",
    "            self.addWord(word)\n",
    "\n",
    "# Create a Voc object from precomputed data structures\n",
    "def loadPrecomputedVoc(corpus_name, word2index_url, index2word_url):\n",
    "    # load the word-to-index lookup map\n",
    "    r = requests.get(word2index_url)\n",
    "    word2index = r.json()\n",
    "    # load the index-to-word lookup map\n",
    "    r = requests.get(index2word_url)\n",
    "    index2word = r.json()\n",
    "    return Voc(corpus_name, word2index, index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import unicodedata\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = nltk.tokenize.RegexpTokenizer(pattern=r'\\w+|[^\\w\\s]')\n",
    "        WORD2INDEX_URL = \"http://zissou.infosci.cornell.edu/convokit/models/craft_wikiconv/word2index.json\"\n",
    "        INDEX2WORD_URL = \"http://zissou.infosci.cornell.edu/convokit/models/craft_wikiconv/index2word.json\"\n",
    "        self.vocab = self.loadPrecomputedVoc(\"wikiconv\", WORD2INDEX_URL, INDEX2WORD_URL)\n",
    "        self.vocab_size = self.vocab.num_words\n",
    "    # Create a Voc object from precomputed data structures\n",
    "    def loadPrecomputedVoc(self, corpus_name, word2index_url, index2word_url):\n",
    "        # load the word-to-index lookup map\n",
    "        r = requests.get(word2index_url)\n",
    "        word2index = r.json()\n",
    "        # load the index-to-word lookup map\n",
    "        r = requests.get(index2word_url)\n",
    "        index2word = r.json()\n",
    "        return Voc(corpus_name, word2index, index2word)\n",
    "    def unicodeToAscii(self, utterance):\n",
    "        return ''.join(\n",
    "            c for c in unicodedata.normalize('NFD', utterance)\n",
    "            if unicodedata.category(c) != 'Mn')\n",
    "    def tokenize(self, utterance):\n",
    "        # simplify the problem space by considering only ASCII data\n",
    "        cleaned_text = self.unicodeToAscii(utterance.lower())\n",
    "        # if the resulting string is empty, nothing else to do\n",
    "        if not cleaned_text.strip():\n",
    "            return []\n",
    "        return self.tokenizer.tokenize(cleaned_text)\n",
    "    \n",
    "    def forward(self, utterance):\n",
    "        tokens = self.tokenize(utterance)\n",
    "        inputs = []\n",
    "        for token in tokens:\n",
    "            if token in self.vocab.word2index:\n",
    "                inputs.append(self.vocab.word2index[token])\n",
    "            else:\n",
    "                inputs.append(self.vocab.UNK_token)\n",
    "        inputs.append(self.vocab.EOS_token)\n",
    "        return inputs\n",
    "    \n",
    "class EncoderRNN(nn.Module):\n",
    "    \"\"\"This module represents the utterance encoder component of CRAFT, responsible for creating vector representations of utterances\"\"\"\n",
    "    def __init__(self, device, hidden_size, embedding_dim, max_utterance_len, batch_size=256, n_layers=1, dropout=0):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.tokenizer = Tokenizer()\n",
    "        self.max_utterance_len = max_utterance_len\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.embedding = nn.Embedding(self.tokenizer.vocab_size, embedding_dim).to(device)\n",
    "\n",
    "        # Initialize GRU; the input_size and hidden_size params are both set to 'hidden_size'\n",
    "        #   because our input size is a word embedding with number of features == hidden_size\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_size, n_layers, batch_first = True,\n",
    "                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True).to(device)\n",
    "    def tokenize(self, utterances):\n",
    "        inputs = [self.tokenizer.forward(utterance) for utterance in utterances]\n",
    "        features = np.zeros((len(inputs), self.max_utterance_len), dtype=int)\n",
    "\n",
    "        # for each review, I grab that review and \n",
    "        for i, row in enumerate(inputs):\n",
    "            features[i, -min(len(row), self.max_utterance_len):] = np.array(row)[:self.max_utterance_len]\n",
    "\n",
    "        def batch(features, batch_size):\n",
    "            for i in range(0, len(features), batch_size):\n",
    "                yield torch.from_numpy(features[i:min(i+batch_size, len(features))]).long().to(self.device)\n",
    "\n",
    "        return batch(features, self.batch_size)\n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = [weight.new(self.n_layers, batch_size, self.hidden_size).zero_() for _ in range(self.n_layers)]\n",
    "        return torch.cat(hidden, dim=0).to(self.device)\n",
    "    def forward(self, utterances, hidden=None):\n",
    "        dataloader = self.tokenize(utterances)\n",
    "        outputs = []\n",
    "        hiddens = []\n",
    "        for inputs in dataloader:\n",
    "            inputs = inputs.to(self.device)\n",
    "            hidden = self.init_hidden(inputs.shape[0])\n",
    "            embedded = self.embedding(inputs)\n",
    "            output, hidden = self.gru(embedded, hidden)\n",
    "            outputs.append(output.to('cpu').detach())\n",
    "            hiddens.append(hidden.to('cpu').detach())\n",
    "        outputs = torch.cat(outputs, dim=0)\n",
    "        hiddens = torch.cat(hiddens, dim=1)\n",
    "        return outputs, hiddens\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wget\n",
    "def load_Utterance_pretrain(device):\n",
    "    hidden_size = 500\n",
    "    encoder_n_layers = 2\n",
    "    dropout = 0.1\n",
    "    # MODEL_URL = \"http://zissou.infosci.cornell.edu/convokit/models/craft_wikiconv/craft_pretrained.tar\"\n",
    "    # print(\"Loading saved parameters...\")\n",
    "    # if not os.path.isfile(\"pretrained_model.tar\"):\n",
    "    #     print(\"\\tDownloading pre-trained CRAFT...\")\n",
    "    #     wget.download(MODEL_URL)\n",
    "    #     # urlretrieve(MODEL_URL, \"craft_pretrained.tar\")\n",
    "    #     print(\"\\t...Done!\")\n",
    "    checkpoint = torch.load(\"pretrained_model.tar\")\n",
    "    encoder = EncoderRNN(device=device, hidden_size=hidden_size, embedding_dim=hidden_size, max_utterance_len = 300,\n",
    "                          n_layers=encoder_n_layers, dropout=dropout)\n",
    "    encoder_sd = checkpoint['en']\n",
    "    embedding_sd = checkpoint['embedding']\n",
    "    encoder.embedding.load_state_dict(embedding_sd)\n",
    "    encoder.load_state_dict(encoder_sd)\n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utterance BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextEncoderRNN(nn.Module):\n",
    "    \"\"\"This module represents the context encoder component of CRAFT, responsible for creating an order-sensitive vector representation of conversation context\"\"\"\n",
    "    def __init__(self, hidden_size, n_layers=1, dropout=0):\n",
    "        super(ContextEncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # only unidirectional GRU for context encoding\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, batch_first = True,\n",
    "                          dropout=(0 if n_layers == 1 else dropout), bidirectional=False)\n",
    "\n",
    "    def forward(self, input_seq, hidden=None):\n",
    "        # Pack padded batch of sequences for RNN module\n",
    "        # packed = torch.nn.utils.rnn.pack_padded_sequence(input_seq, input_lengths)\n",
    "        # Forward pass through GRU\n",
    "        outputs, hidden = self.gru(input_seq, hidden)\n",
    "        # Unpack padding\n",
    "        # outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        # return output and final hidden state\n",
    "        return outputs, hidden\n",
    "\n",
    "class SingleTargetClf(nn.Module):\n",
    "    \"\"\"This module represents the CRAFT classifier head, which takes the context encoding and uses it to make a forecast\"\"\"\n",
    "    def __init__(self, hidden_size, dropout=0.1):\n",
    "        super(SingleTargetClf, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # initialize classifier\n",
    "        self.layer1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.layer1_act = nn.LeakyReLU()\n",
    "        self.layer2 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.layer2_act = nn.LeakyReLU()\n",
    "        self.clf = nn.Linear(hidden_size // 2, 1)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    # def forward(self, encoder_outputs, encoder_input_lengths):\n",
    "    #     # from stackoverflow (https://stackoverflow.com/questions/50856936/taking-the-last-state-from-bilstm-bigru-in-pytorch)\n",
    "    #     # First we unsqueeze seqlengths two times so it has the same number of\n",
    "    #     # of dimensions as output_forward\n",
    "    #     # (batch_size) -> (1, batch_size, 1)\n",
    "    #     lengths = encoder_input_lengths.unsqueeze(0).unsqueeze(2)\n",
    "    #     # Then we expand it accordingly\n",
    "    #     # (1, batch_size, 1) -> (1, batch_size, hidden_size)\n",
    "    #     lengths = lengths.expand((1, -1, encoder_outputs.size(2)))\n",
    "\n",
    "    #     # take only the last state of the encoder for each batch\n",
    "    #     last_outputs = torch.gather(encoder_outputs, 0, lengths-1).squeeze()\n",
    "    #     # forward pass through hidden layers\n",
    "    #     layer1_out = self.layer1_act(self.layer1(self.dropout(last_outputs)))\n",
    "    #     layer2_out = self.layer2_act(self.layer2(self.dropout(layer1_out)))\n",
    "    #     # compute and return logits\n",
    "    #     logits = self.clf(self.dropout(layer2_out)).squeeze()\n",
    "    #     return logits\n",
    "    def forward(self, encoder_hidden):\n",
    "        hidden = encoder_hidden[-1,:,:]\n",
    "        # forward pass through hidden layers\n",
    "        hidden = hidden.squeeze()\n",
    "        layer1_out = self.layer1_act(self.layer1(self.dropout(hidden)))\n",
    "        layer2_out = self.layer2_act(self.layer2(self.dropout(layer1_out)))\n",
    "        # compute and return logits\n",
    "        logits = self.clf(self.dropout(layer2_out)).squeeze()\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_size = 500\n",
    "context_encoder_n_layers = 2\n",
    "dropout = 0.1\n",
    "context_encoder = ContextEncoderRNN(hidden_size, context_encoder_n_layers, dropout)\n",
    "checkpoint = torch.load(\"pretrained_model.tar\")\n",
    "context_sd = checkpoint['ctx']\n",
    "context_encoder.load_state_dict(context_sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_clf = SingleTargetClf(hidden_size, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_encoder = context_encoder.to(device)\n",
    "attack_clf = attack_clf.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_utt = load_Utterance_pretrain(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_utterances, train_conversationLength, train_comment_ids, train_labels = load_data(corpus, split='train', last_only=True, context_batch_size=99999999)\n",
    "valid_utterances, valid_conversationLength, valid_comment_ids, valid_labels = load_data(corpus, split='val', last_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2508\n",
      "0.5\n",
      "0.4375\n"
     ]
    }
   ],
   "source": [
    "print(len(train_labels[0]))\n",
    "print(sum(train_labels[0])/len(train_labels[0]))\n",
    "print(sum(valid_labels[0])/len(valid_labels[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_context_batch(utt_hidden, batch_conversationLength, max_context_len=20):\n",
    "    assert utt_hidden.shape[1] == sum(batch_conversationLength)\n",
    "    utt_encoder_summed = utt_hidden[-2,:,:] + utt_hidden[-1,:,:]\n",
    "    hidden_size = utt_encoder_summed.shape[1]\n",
    "    context_features = np.zeros((len(batch_conversationLength), max_context_len, hidden_size), dtype=np.float32)\n",
    "\n",
    "    current_utt_idx = 0\n",
    "    for i, convo_len in enumerate(batch_conversationLength):\n",
    "        if convo_len > max_context_len:\n",
    "            current_utt_idx += convo_len - max_context_len\n",
    "            convo_len = max_context_len\n",
    "        context_features[i, -convo_len:, :] = np.array(utt_encoder_summed)[current_utt_idx:current_utt_idx+convo_len, :]\n",
    "        current_utt_idx += convo_len\n",
    "    return context_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/Users/tran_s2/.local/lib/python3.11/site-packages/huggingface_hub/utils/_runtime.py:184: UserWarning: Pydantic is installed but cannot be imported. Please check your installation. `huggingface_hub` will default to not using Pydantic. Error message: '{e}'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "\n",
    "encoder_optimizer = optim.Adam(my_utt.parameters(), lr=learning_rate)\n",
    "context_encoder_optimizer = optim.Adam(context_encoder.parameters(), lr=learning_rate)\n",
    "attack_clf_optimizer = optim.Adam(attack_clf.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.nn import BCEWithLogitsLoss\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score\n",
    "def calculate_f1_score(labels, preds):\n",
    "    # preds = torch.sigmoid(logits) > 0.5\n",
    "    # Calculating precision, recall, and F1 score using PyTorch\n",
    "    TP = ((preds == 1) & (labels == 1)).sum().item()\n",
    "    FP = ((preds == 1) & (labels == 0)).sum().item()\n",
    "    FN = ((preds == 0) & (labels == 1)).sum().item()\n",
    "\n",
    "    precision = TP / (TP + FP) if TP + FP > 0 else 0\n",
    "    recall = TP / (TP + FN) if TP + FN > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return f1\n",
    "def evaluate(encoder, context_encoder, attack_clf, val_utterances, val_conversationLength, val_labels):\n",
    "    encoder.eval()\n",
    "    context_encoder.eval()\n",
    "    attack_clf.eval()\n",
    "    val_loss = 0\n",
    "    val_f1 = 0\n",
    "    val_accuracy = 0\n",
    "    val_batches = 0\n",
    "    pos = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    for i in range(len(val_utterances)):\n",
    "        batch_utterances = val_utterances[i]\n",
    "        batch_conversationLength = val_conversationLength[i]\n",
    "        batch_labels = val_labels[i]\n",
    "        batch_size = len(batch_labels)\n",
    "        if batch_size == 0:\n",
    "            continue\n",
    "        with torch.no_grad():\n",
    "            utt_outputs, utt_hidden = encoder.forward(batch_utterances)\n",
    "            context_features = prepare_context_batch(utt_hidden, batch_conversationLength)\n",
    "            context_features = torch.from_numpy(context_features).to(device)\n",
    "            context_outputs, context_hidden = context_encoder(context_features)\n",
    "            logits = attack_clf(context_hidden)\n",
    "            labels = torch.tensor(batch_labels, dtype=torch.float32).to(device)\n",
    "            # pos_weight = torch.tensor([1]).type_as(logits)\n",
    "            # loss_fct = BCEWithLogitsLoss(pos_weight=pos_weight, reduction = 'sum')\n",
    "            # loss = loss_fct(logits, labels)\n",
    "            loss = F.binary_cross_entropy_with_logits(logits, labels)\n",
    "            val_loss += loss.item()\n",
    "            preds = torch.sigmoid(logits) > 0.5\n",
    "            pos += preds.sum().item() / len(batch_labels)\n",
    "            val_f1 += calculate_f1_score(labels.cpu().detach(), logits.cpu().detach())\n",
    "            val_accuracy += (preds == labels).sum().item() / len(batch_labels)\n",
    "            val_batches += 1\n",
    "            all_labels.append(labels.cpu().detach())\n",
    "            all_preds.append(preds.cpu().detach())\n",
    "    all_labels = torch.cat(all_labels)\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    val_f1 = calculate_f1_score(all_labels, all_preds)\n",
    "    return val_loss / val_batches, val_f1, val_accuracy/val_batches, pos/len(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 0\n",
      "Validation loss: 0.69 accuracy: 49.31 f1: 66.67 pos: 1.00\n",
      "100 1\n",
      "Validation loss: 0.69 accuracy: 49.31 f1: 66.67 pos: 1.00\n",
      "150 1\n",
      "Validation loss: 0.69 accuracy: 50.00 f1: 66.93 pos: 0.99\n",
      "200 2\n",
      "Validation loss: 0.69 accuracy: 53.82 f1: 67.24 pos: 0.89\n",
      "250 3\n",
      "Validation loss: 0.69 accuracy: 53.01 f1: 66.21 pos: 0.88\n",
      "300 3\n",
      "Validation loss: 0.69 accuracy: 58.10 f1: 59.49 pos: 0.53\n",
      "350 4\n",
      "Validation loss: 0.69 accuracy: 58.10 f1: 62.24 pos: 0.59\n",
      "400 5\n",
      "Validation loss: 0.69 accuracy: 56.60 f1: 62.26 pos: 0.64\n",
      "450 5\n",
      "Validation loss: 0.69 accuracy: 58.80 f1: 56.68 pos: 0.44\n",
      "500 6\n",
      "Validation loss: 0.68 accuracy: 57.87 f1: 59.18 pos: 0.51\n",
      "550 6\n",
      "Validation loss: 0.68 accuracy: 58.80 f1: 60.28 pos: 0.53\n",
      "600 7\n",
      "Validation loss: 0.68 accuracy: 58.91 f1: 56.64 pos: 0.44\n",
      "650 8\n",
      "Validation loss: 0.68 accuracy: 59.38 f1: 62.12 pos: 0.56\n",
      "700 8\n",
      "Validation loss: 0.68 accuracy: 59.14 f1: 53.98 pos: 0.38\n",
      "750 9\n",
      "Validation loss: 0.67 accuracy: 60.07 f1: 58.43 pos: 0.45\n",
      "800 10\n",
      "Validation loss: 0.67 accuracy: 59.38 f1: 62.08 pos: 0.57\n",
      "850 10\n",
      "Validation loss: 0.67 accuracy: 59.72 f1: 56.36 pos: 0.41\n",
      "900 11\n",
      "Validation loss: 0.67 accuracy: 59.95 f1: 53.99 pos: 0.36\n",
      "950 12\n",
      "Validation loss: 0.66 accuracy: 59.61 f1: 63.57 pos: 0.60\n",
      "1000 12\n",
      "Validation loss: 0.66 accuracy: 60.19 f1: 55.01 pos: 0.38\n",
      "1050 13\n",
      "Validation loss: 0.66 accuracy: 63.19 f1: 60.54 pos: 0.43\n",
      "1100 13\n",
      "Validation loss: 0.66 accuracy: 61.69 f1: 64.52 pos: 0.58\n",
      "1150 14\n",
      "Validation loss: 0.66 accuracy: 62.50 f1: 63.97 pos: 0.53\n",
      "1200 15\n",
      "Validation loss: 0.66 accuracy: 63.66 f1: 61.92 pos: 0.45\n",
      "1250 15\n",
      "Validation loss: 0.66 accuracy: 63.66 f1: 61.25 pos: 0.44\n",
      "1300 16\n",
      "Validation loss: 0.66 accuracy: 63.08 f1: 64.59 pos: 0.54\n",
      "1350 17\n",
      "Validation loss: 0.66 accuracy: 62.62 f1: 59.97 pos: 0.42\n",
      "1400 17\n",
      "Validation loss: 0.67 accuracy: 60.88 f1: 54.47 pos: 0.35\n",
      "1450 18\n",
      "Validation loss: 0.66 accuracy: 62.38 f1: 58.76 pos: 0.40\n",
      "1500 18\n",
      "Validation loss: 0.66 accuracy: 63.08 f1: 66.59 pos: 0.60\n",
      "1550 19\n",
      "Validation loss: 0.66 accuracy: 62.62 f1: 59.66 pos: 0.41\n",
      "1600 20\n",
      "Validation loss: 0.66 accuracy: 62.85 f1: 63.22 pos: 0.49\n",
      "1650 20\n",
      "Validation loss: 0.66 accuracy: 63.77 f1: 65.28 pos: 0.54\n",
      "1700 21\n",
      "Validation loss: 0.66 accuracy: 64.00 f1: 65.98 pos: 0.55\n",
      "1750 22\n",
      "Validation loss: 0.66 accuracy: 62.50 f1: 60.10 pos: 0.43\n",
      "1800 22\n",
      "Validation loss: 0.66 accuracy: 62.38 f1: 60.96 pos: 0.44\n",
      "1850 23\n",
      "Validation loss: 0.66 accuracy: 62.38 f1: 62.01 pos: 0.47\n",
      "1900 24\n",
      "Validation loss: 0.66 accuracy: 62.85 f1: 63.04 pos: 0.48\n",
      "1950 24\n",
      "Validation loss: 0.69 accuracy: 60.07 f1: 52.63 pos: 0.33\n",
      "2000 25\n",
      "Validation loss: 0.66 accuracy: 62.85 f1: 62.77 pos: 0.48\n",
      "2050 25\n",
      "Validation loss: 0.67 accuracy: 63.54 f1: 66.59 pos: 0.57\n",
      "2100 26\n",
      "Validation loss: 0.67 accuracy: 63.54 f1: 64.20 pos: 0.49\n",
      "2150 27\n",
      "Validation loss: 0.67 accuracy: 63.08 f1: 62.93 pos: 0.47\n",
      "2200 27\n",
      "Validation loss: 0.67 accuracy: 64.35 f1: 65.73 pos: 0.51\n",
      "2250 28\n",
      "Validation loss: 0.67 accuracy: 63.77 f1: 64.78 pos: 0.50\n",
      "2300 29\n",
      "Validation loss: 0.67 accuracy: 61.00 f1: 57.74 pos: 0.41\n",
      "2350 29\n",
      "Validation loss: 0.67 accuracy: 62.04 f1: 60.33 pos: 0.44\n"
     ]
    }
   ],
   "source": [
    "num_steps = 0\n",
    "for epoch in range(30):\n",
    "    for batch_idx in range(len(train_labels)):\n",
    "        num_steps += 1\n",
    "        if num_steps % 50 == 0:\n",
    "            print(num_steps, epoch)\n",
    "            val_loss, val_f1, val_accuracy, pos= evaluate(my_utt, context_encoder, attack_clf, valid_utterances, valid_conversationLength, valid_labels)\n",
    "            print(\"Validation loss: {:.2f} accuracy: {:.2f} f1: {:.2f} pos: {:.2f}\".format(val_loss, val_accuracy * 100, val_f1 * 100, pos))\n",
    "        my_utt.train()\n",
    "        context_encoder.train()\n",
    "        attack_clf.train()\n",
    "        \n",
    "        batch_utterances = train_utterances[batch_idx]\n",
    "        batch_conversationLength = train_conversationLength[batch_idx]\n",
    "        batch_comment_ids = train_comment_ids[batch_idx]\n",
    "        batch_labels = train_labels[batch_idx]\n",
    "        encoder_optimizer.zero_grad()\n",
    "        context_encoder_optimizer.zero_grad()\n",
    "        attack_clf_optimizer.zero_grad()\n",
    "        \n",
    "        encoder_outputs, hidden = my_utt.forward(batch_utterances)\n",
    "        context_features = torch.from_numpy(prepare_context_batch(hidden, batch_conversationLength)).to(device)\n",
    "        final_outputs, final_hidden = context_encoder.forward(context_features)\n",
    "        logits = attack_clf(final_hidden)\n",
    "        labels = torch.tensor(batch_labels, dtype=torch.float32).to(device)\n",
    "        # pos_weight = torch.tensor([1]).type_as(logits)\n",
    "        # loss_fct = BCEWithLogitsLoss(pos_weight=pos_weight, reduction = 'sum')\n",
    "        # loss = loss_fct(logits, labels)\n",
    "        loss = F.binary_cross_entropy_with_logits(logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        clip = 50.0\n",
    "        # Clip gradients: gradients are modified in place\n",
    "        _ = torch.nn.utils.clip_grad_norm_(my_utt.parameters(), clip)\n",
    "        _ = torch.nn.utils.clip_grad_norm_(context_encoder.parameters(), clip)\n",
    "        _ = torch.nn.utils.clip_grad_norm_(attack_clf.parameters(), clip)\n",
    "\n",
    "        # Adjust model weights\n",
    "        encoder_optimizer.step()\n",
    "        context_encoder_optimizer.step()\n",
    "        attack_clf_optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
